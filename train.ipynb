{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from time import time\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Environment Wrapper to Handle Different Environments\n",
    "class EnvironmentWrapper:\n",
    "    def __init__(self, env_name, state_pad, action_pad):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_pad = state_pad\n",
    "        self.action_pad = action_pad\n",
    "        \n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        return np.append(state, np.zeros(np.max((self.state_pad - len(state), 0))))\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        state = np.append(state, np.zeros(np.max((self.state_pad - len(state), 0))))\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "# Define the Policy (Actor) and Value (Critic) Networks\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, state_size, action_size, env_name, lr_actor=0.001, lr_critic=0.0005, verbosity=10):\n",
    "        self.policy_network = PolicyNetwork(state_size, action_size).to(device)\n",
    "        self.value_network = ValueNetwork(state_size).to(device)\n",
    "        self.optimizer_actor = optim.Adam(self.policy_network.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.value_network.parameters(), lr=lr_critic)\n",
    "        self.verbosity = verbosity\n",
    "        self.env_name =  env_name\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy_network(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def update_policy(self, transitions, gamma=0.99):\n",
    "        loss_policy = 0\n",
    "        loss_value = 0\n",
    "\n",
    "        for transition in transitions:\n",
    "            state, action, reward, next_state, done = transition\n",
    "\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            action = torch.tensor(action).view(1, -1).to(device)\n",
    "            reward = torch.tensor(reward).float().to(device)\n",
    "            done = torch.tensor(done).float().to(device)\n",
    "\n",
    "            # Compute value loss\n",
    "            predicted_value = self.value_network(state)\n",
    "            next_predicted_value = self.value_network(next_state)\n",
    "            expected_value = reward + gamma * next_predicted_value * (1 - done)\n",
    "            loss_value += nn.MSELoss()(predicted_value, expected_value.detach())\n",
    "\n",
    "            # Compute policy loss\n",
    "            _, log_prob = self.select_action(state)\n",
    "            advantage = expected_value - predicted_value.detach()\n",
    "            loss_policy += -log_prob * advantage\n",
    "\n",
    "        # Backpropagate losses\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss_value.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        return loss_policy.item(), loss_value.item()\n",
    "\n",
    "    def save_models(self, path='models'):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.policy_network.state_dict(), os.path.join(path, f'{self.env_name}_policy_network.pth'))\n",
    "        torch.save(self.value_network.state_dict(), os.path.join(path, f'{self.env_name}_value_network.pth'))\n",
    "\n",
    "    def load_models(self, path='models'):\n",
    "        self.policy_network.load_state_dict(torch.load(os.path.join(path, f'{self.env_name}_policy_network.pth'), map_location=device))\n",
    "        self.value_network.load_state_dict(torch.load(os.path.join(path, f'{self.env_name}_value_network.pth'), map_location=device))\n",
    "\n",
    "\n",
    "    def train(self, env_wrapper, max_episodes=1000, max_steps=500, reward_threshold=475.0):\n",
    "        self.results = {'Episode': [], 'Reward': [], \"Average_100\": [], 'Solved': -1, 'Duration': 0, 'Loss': [], 'LossV': []}\n",
    "        results = self.results\n",
    "        start_time = time()\n",
    "        episode_rewards = []\n",
    "\n",
    "        transitions = []\n",
    "        for episode in range(max_episodes):\n",
    "            state = env_wrapper.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                next_state, reward, done, _ = env_wrapper.step(action)\n",
    "                transitions.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                if len(transitions) == 100: # update weights every 100 steps\n",
    "                    loss_policy, loss_value = self.update_policy(transitions)\n",
    "                    results['Loss'].append(loss_policy)\n",
    "                    results['LossV'].append(loss_value)\n",
    "                    transitions = transitions [50:]\n",
    "\n",
    "            loss_policy, loss_value = self.update_policy(transitions)\n",
    "            results['Loss'].append(loss_policy)\n",
    "            results['LossV'].append(loss_value)            \n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "            results['Episode'].append(episode)\n",
    "            results['Reward'].append(episode_reward)\n",
    "\n",
    "            if len(episode_rewards) >= 100:\n",
    "                avg_reward = sum(episode_rewards[-100:]) / 100\n",
    "                results['Average_100'].append(avg_reward)\n",
    "                if avg_reward > reward_threshold and results['Solved'] == -1:\n",
    "                    results['Solved'] = episode\n",
    "                    print(f\"Solved at episode {episode} with average reward {avg_reward}.\")\n",
    "                    break\n",
    "            else:\n",
    "                results['Average_100'].append(sum(episode_rewards) / len(episode_rewards))\n",
    "\n",
    "            if episode % self.verbosity == 0:\n",
    "                print(f\"Episode {episode}, Avg Reward: {results['Average_100'][-1]}, PLoss: {loss_policy}, VLoss: {loss_value}\")\n",
    "\n",
    "        results['Duration'] = time() - start_time\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 24.0, PLoss: 16.750720977783203, VLoss: 24.097936630249023\n",
      "Episode 50, Avg Reward: 25.294117647058822, PLoss: 60.443931579589844, VLoss: 86.26277923583984\n",
      "Episode 100, Avg Reward: 24.88, PLoss: 809.0989990234375, VLoss: 1381.7835693359375\n",
      "Episode 150, Avg Reward: 23.24, PLoss: 1378.4110107421875, VLoss: 4726.27001953125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m action_size \u001b[38;5;241m=\u001b[39m env_wrapper\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m action_pad\n\u001b[0;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m ActorCriticAgent(state_size, action_size, env_name, verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n\u001b[1;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Save the trained models\u001b[39;00m\n\u001b[0;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_models()\n",
      "Cell \u001b[1;32mIn[12], line 157\u001b[0m, in \u001b[0;36mActorCriticAgent.train\u001b[1;34m(self, env_wrapper, max_episodes, max_steps, reward_threshold)\u001b[0m\n\u001b[0;32m    154\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[0;32m    155\u001b[0m         transitions \u001b[38;5;241m=\u001b[39m transitions [\u001b[38;5;241m50\u001b[39m:]\n\u001b[1;32m--> 157\u001b[0m loss_policy, loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_policy)\n\u001b[0;32m    159\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_value)            \n",
      "Cell \u001b[1;32mIn[12], line 96\u001b[0m, in \u001b[0;36mActorCriticAgent.update_policy\u001b[1;34m(self, transitions, gamma)\u001b[0m\n\u001b[0;32m     93\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Compute value loss\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m predicted_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m next_predicted_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_network(next_state)\n\u001b[0;32m     98\u001b[0m expected_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_predicted_value \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m, in \u001b[0;36mValueNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\miniconda3\\envs\\nnenv\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example environment setup\n",
    "env_name = 'CartPole-v1'  # This can be replaced with 'Acrobot-v1' or 'MountainCarContinuous-v0'\n",
    "state_pad = 0  # Adjust based on the environment\n",
    "action_pad = 0  # Adjust based on the environment\n",
    "verbosity = 50\n",
    "\n",
    "env_wrapper = EnvironmentWrapper(env_name, state_pad, action_pad)\n",
    "state_size = env_wrapper.env.observation_space.shape[0] + state_pad\n",
    "action_size = env_wrapper.env.action_space.n + action_pad\n",
    "\n",
    "agent = ActorCriticAgent(state_size, action_size, env_name, verbosity=verbosity)\n",
    "results = agent.train(env_wrapper, max_episodes=1000)\n",
    "\n",
    "# Save the trained models\n",
    "agent.save_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mepisode_rewards\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'episode_rewards' is not defined"
     ]
    }
   ],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
