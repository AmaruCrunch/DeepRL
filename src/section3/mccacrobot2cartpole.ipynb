{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from actorcritic import Network, ActorCriticAgent, EnvironmentWrapper  \n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a model\n",
    "def load_model(model_path, input_size, output_size, hidden_sizes, is_policy, device=device):\n",
    "    model = Network(input_size, output_size, hidden_sizes, is_policy)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model.to(device)\n",
    "\n",
    "# Function to freeze a network\n",
    "def freeze_network(network):\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Adapter, self).__init__()\n",
    "        # Define a simple MLP with one hidden layer for dimensionality adaptation and non-linearity\n",
    "        self.adapter_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),  # You might want to change the size or add more layers\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.adapter_layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveNetwork(nn.Module):\n",
    "    def __init__(self, input_size=6, output_size=3, hidden_sizes=[64, 64], adapter_size=2, source_models=[], is_policy=False, device=torch.device('cpu')):\n",
    "        super(ProgressiveNetwork, self).__init__()\n",
    "        self.source_models = source_models\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.is_policy = is_policy\n",
    "\n",
    "        # Initialize adapters for each layer of the source models\n",
    "        self.adapters = [nn.ModuleList() for _ in range(len(source_models))]\n",
    "        for i, source_model in enumerate(source_models):\n",
    "            adapter1 = Adapter(source_model.hidden_sizes[0], adapter_size).to(device)\n",
    "            adapter2 = Adapter(source_model.hidden_sizes[1], adapter_size).to(device)\n",
    "            self.adapters[i].append(adapter1)\n",
    "            self.adapters[i].append(adapter2)\n",
    "\n",
    "        # For 2 hidden layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.hidden_layer = nn.Linear(hidden_sizes[0] + len(source_models) * adapter_size, \n",
    "                                      hidden_sizes[1])  \n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_sizes[1] + len(source_models) * adapter_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get ouput from first layer\n",
    "        output = F.relu(self.input_layer(x))\n",
    "\n",
    "        # for model in self.source_models, get output from first layer:\n",
    "        for i, model in enumerate(self.source_models):\n",
    "            model_output = model.network[:2](x)\n",
    "            # Apply adapter to the output\n",
    "            model_output = self.adapters[i][0](model_output)\n",
    "            output = torch.cat((output, model_output), dim=-1)\n",
    "\n",
    "        # repeat for second layer\n",
    "        # get ouput from first layer\n",
    "        output = F.relu(self.hidden_layer(output))\n",
    "\n",
    "        # for model in self.source_models, get output from first layer:\n",
    "        for i, model in enumerate(self.source_models):\n",
    "            model_output = model.network[:4](x)\n",
    "            # Apply adapter to the output\n",
    "            model_output = self.adapters[i][1](model_output)\n",
    "            output = torch.cat((output, model_output), dim=-1)\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        if self.is_policy:\n",
    "            return nn.Softmax(dim=-1)(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting up and using the Progressive Network\n",
    "# Assuming the input_size, output_size, and hidden_sizes are defined elsewhere\n",
    "source_networks = [load_model('models/Acrobot-v1_policy_network.pth', 6, 3, [64, 64], True),\n",
    "                   load_model('models/MountainCarContinuous-v0_policy_network.pth', 6, 3, [64, 64], True)]\n",
    "\n",
    "# Freeze the source networks\n",
    "for net in source_networks:\n",
    "    freeze_network(net)\n",
    "\n",
    "# Setup Progressive Network for CartPole (or another target task)\n",
    "# You need to define cartpole_input_size, cartpole_output_size, and whether it's a policy or value network accordingly\n",
    "policy_network = ProgressiveNetwork(input_size=6, output_size=3, hidden_sizes=[64, 64], adapter_size=2, source_models=source_networks, is_policy=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting up and using the Progressive Network\n",
    "# Assuming the input_size, output_size, and hidden_sizes are defined elsewhere\n",
    "source_networks = [load_model('models/Acrobot-v1_value_network.pth', 6, 1, [64, 64], False),\n",
    "                   load_model('models/MountainCarContinuous-v0_value_network.pth', 6, 1, [64, 64], False)]\n",
    "\n",
    "# Freeze the source networks\n",
    "for net in source_networks:\n",
    "    freeze_network(net)\n",
    "\n",
    "# Setup Progressive Network for CartPole (or another target task)\n",
    "# You need to define cartpole_input_size, cartpole_output_size, and whether it's a policy or value network accordingly\n",
    "value_network = ProgressiveNetwork(input_size=6, output_size=1, hidden_sizes=[64, 64], adapter_size=2, source_models=source_networks, is_policy=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "config = {\n",
    "    'experiment': 'ProgressiveCartPole',\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 64], \n",
    "    'lr_actor': 0.001,\n",
    "    'lr_critic': 0.005,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'CartPole-v1',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 475.0,\n",
    "    'max_episodes': 2000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 500\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = EnvironmentWrapper(env)\n",
    "\n",
    "# Initialize the ActorCriticAgent\n",
    "agent = ActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config['device'])\n",
    "agent.policy_network = policy_network.to(device)\n",
    "agent.value_network = value_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 24.0, PLoss: 25.374746322631836, VLoss: 24.224069595336914\n",
      "Episode 10, Avg Reward: 22.727272727272727, PLoss: 17.677387237548828, VLoss: 16.180065155029297\n",
      "Episode 20, Avg Reward: 20.0, PLoss: 22.273283004760742, VLoss: 21.151987075805664\n",
      "Episode 30, Avg Reward: 18.548387096774192, PLoss: 11.310555458068848, VLoss: 10.14748764038086\n",
      "Episode 40, Avg Reward: 18.682926829268293, PLoss: 16.089935302734375, VLoss: 15.144342422485352\n",
      "Episode 50, Avg Reward: 19.45098039215686, PLoss: 14.265267372131348, VLoss: 13.189603805541992\n",
      "Episode 60, Avg Reward: 18.540983606557376, PLoss: 9.83056354522705, VLoss: 10.174091339111328\n",
      "Episode 70, Avg Reward: 17.830985915492956, PLoss: 17.141817092895508, VLoss: 16.17665672302246\n",
      "Episode 80, Avg Reward: 17.91358024691358, PLoss: 13.511711120605469, VLoss: 13.202524185180664\n",
      "Episode 90, Avg Reward: 17.791208791208792, PLoss: 45.274593353271484, VLoss: 40.23586654663086\n",
      "Episode 100, Avg Reward: 17.69, PLoss: 22.799264907836914, VLoss: 21.178632736206055\n",
      "Episode 110, Avg Reward: 17.09, PLoss: 19.473108291625977, VLoss: 18.168136596679688\n",
      "Episode 120, Avg Reward: 17.01, PLoss: 17.30579376220703, VLoss: 16.18009376525879\n",
      "Episode 130, Avg Reward: 17.23, PLoss: 33.73886489868164, VLoss: 30.215042114257812\n",
      "Episode 140, Avg Reward: 16.77, PLoss: 11.260584831237793, VLoss: 10.146448135375977\n",
      "Episode 150, Avg Reward: 16.08, PLoss: 15.99688720703125, VLoss: 14.193770408630371\n",
      "Episode 160, Avg Reward: 16.36, PLoss: 20.054359436035156, VLoss: 17.16916847229004\n",
      "Episode 170, Avg Reward: 16.41, PLoss: 9.75606918334961, VLoss: 9.15365982055664\n",
      "Episode 180, Avg Reward: 16.77, PLoss: 15.065535545349121, VLoss: 14.195873260498047\n",
      "Episode 190, Avg Reward: 16.53, PLoss: 15.41600513458252, VLoss: 15.166601181030273\n",
      "Episode 200, Avg Reward: 16.53, PLoss: 17.626476287841797, VLoss: 16.195043563842773\n",
      "Episode 210, Avg Reward: 16.38, PLoss: 26.392417907714844, VLoss: 24.117019653320312\n",
      "Episode 220, Avg Reward: 16.39, PLoss: 13.447244644165039, VLoss: 12.15262508392334\n",
      "Episode 230, Avg Reward: 16.2, PLoss: 22.6571044921875, VLoss: 20.191499710083008\n",
      "Episode 240, Avg Reward: 16.41, PLoss: 15.880608558654785, VLoss: 15.163310050964355\n",
      "Episode 250, Avg Reward: 16.56, PLoss: 15.851645469665527, VLoss: 14.156292915344238\n",
      "Episode 260, Avg Reward: 16.46, PLoss: 22.71124839782715, VLoss: 20.20949363708496\n",
      "Episode 270, Avg Reward: 16.67, PLoss: 10.636841773986816, VLoss: 9.159794807434082\n",
      "Episode 280, Avg Reward: 16.28, PLoss: 30.756412506103516, VLoss: 28.213115692138672\n",
      "Episode 290, Avg Reward: 16.64, PLoss: 22.59276008605957, VLoss: 21.177791595458984\n",
      "Episode 300, Avg Reward: 16.48, PLoss: 33.70473861694336, VLoss: 30.188941955566406\n",
      "Episode 310, Avg Reward: 16.4, PLoss: 25.925739288330078, VLoss: 23.194053649902344\n",
      "Episode 320, Avg Reward: 16.47, PLoss: 17.538476943969727, VLoss: 16.169496536254883\n",
      "Episode 330, Avg Reward: 16.36, PLoss: 11.996284484863281, VLoss: 11.177508354187012\n",
      "Episode 340, Avg Reward: 16.29, PLoss: 11.007242202758789, VLoss: 10.167991638183594\n",
      "Episode 350, Avg Reward: 16.1, PLoss: 18.147157669067383, VLoss: 17.16485023498535\n",
      "Episode 360, Avg Reward: 15.97, PLoss: 12.393383979797363, VLoss: 11.157135009765625\n",
      "Episode 370, Avg Reward: 16.39, PLoss: 10.276677131652832, VLoss: 10.15823745727539\n",
      "Episode 380, Avg Reward: 16.04, PLoss: 34.521949768066406, VLoss: 33.22549819946289\n",
      "Episode 390, Avg Reward: 15.74, PLoss: 10.874317169189453, VLoss: 10.16085433959961\n",
      "Episode 400, Avg Reward: 16.1, PLoss: 9.695472717285156, VLoss: 9.156875610351562\n",
      "Episode 410, Avg Reward: 16.48, PLoss: 54.9845085144043, VLoss: 49.250762939453125\n",
      "Episode 420, Avg Reward: 16.11, PLoss: 8.289828300476074, VLoss: 8.151673316955566\n",
      "Episode 430, Avg Reward: 16.32, PLoss: 23.196443557739258, VLoss: 21.185020446777344\n",
      "Episode 440, Avg Reward: 16.2, PLoss: 15.65433406829834, VLoss: 14.16815185546875\n",
      "Episode 450, Avg Reward: 16.43, PLoss: 14.824915885925293, VLoss: 14.154956817626953\n",
      "Episode 460, Avg Reward: 16.54, PLoss: 11.329001426696777, VLoss: 10.14077091217041\n",
      "Episode 470, Avg Reward: 15.97, PLoss: 20.926240921020508, VLoss: 19.16836929321289\n",
      "Episode 480, Avg Reward: 16.22, PLoss: 19.820417404174805, VLoss: 18.17089080810547\n",
      "Episode 490, Avg Reward: 16.02, PLoss: 10.57138442993164, VLoss: 9.171365737915039\n",
      "Episode 500, Avg Reward: 16.07, PLoss: 13.578568458557129, VLoss: 13.196356773376465\n",
      "Episode 510, Avg Reward: 15.7, PLoss: 16.323497772216797, VLoss: 15.206748008728027\n",
      "Episode 520, Avg Reward: 15.83, PLoss: 22.405914306640625, VLoss: 20.200551986694336\n",
      "Episode 530, Avg Reward: 16.0, PLoss: 12.246031761169434, VLoss: 11.157845497131348\n",
      "Episode 540, Avg Reward: 16.94, PLoss: 10.61258602142334, VLoss: 10.179352760314941\n",
      "Episode 550, Avg Reward: 16.86, PLoss: 18.13170623779297, VLoss: 16.203691482543945\n",
      "Episode 560, Avg Reward: 16.6, PLoss: 17.840290069580078, VLoss: 16.186485290527344\n",
      "Episode 570, Avg Reward: 16.74, PLoss: 23.5003604888916, VLoss: 21.15715980529785\n",
      "Episode 580, Avg Reward: 16.68, PLoss: 9.421737670898438, VLoss: 9.158833503723145\n",
      "Episode 590, Avg Reward: 17.05, PLoss: 11.188010215759277, VLoss: 10.144851684570312\n",
      "Episode 600, Avg Reward: 16.84, PLoss: 18.30691146850586, VLoss: 17.178316116333008\n",
      "Episode 610, Avg Reward: 17.15, PLoss: 13.828622817993164, VLoss: 12.161920547485352\n",
      "Episode 620, Avg Reward: 17.2, PLoss: 10.876633644104004, VLoss: 10.160176277160645\n",
      "Episode 630, Avg Reward: 17.7, PLoss: 17.545644760131836, VLoss: 16.190521240234375\n",
      "Episode 640, Avg Reward: 17.02, PLoss: 15.421245574951172, VLoss: 14.152780532836914\n",
      "Episode 650, Avg Reward: 17.52, PLoss: 9.101678848266602, VLoss: 9.17746353149414\n",
      "Episode 660, Avg Reward: 17.47, PLoss: 10.043048858642578, VLoss: 9.145914077758789\n",
      "Episode 670, Avg Reward: 17.81, PLoss: 26.583538055419922, VLoss: 24.138456344604492\n",
      "Episode 680, Avg Reward: 17.63, PLoss: 21.705768585205078, VLoss: 19.202905654907227\n",
      "Episode 690, Avg Reward: 17.4, PLoss: 13.631281852722168, VLoss: 12.18668270111084\n",
      "Episode 700, Avg Reward: 17.12, PLoss: 11.936982154846191, VLoss: 11.198081016540527\n",
      "Episode 710, Avg Reward: 17.05, PLoss: 11.841002464294434, VLoss: 11.159041404724121\n",
      "Episode 720, Avg Reward: 16.89, PLoss: 11.442487716674805, VLoss: 10.163311004638672\n",
      "Episode 730, Avg Reward: 16.32, PLoss: 27.479352951049805, VLoss: 26.232980728149414\n",
      "Episode 740, Avg Reward: 16.0, PLoss: 12.632399559020996, VLoss: 12.192935943603516\n",
      "Episode 750, Avg Reward: 15.24, PLoss: 9.787094116210938, VLoss: 9.170483589172363\n",
      "Episode 760, Avg Reward: 15.56, PLoss: 12.066585540771484, VLoss: 11.140788078308105\n",
      "Episode 770, Avg Reward: 15.52, PLoss: 17.490610122680664, VLoss: 16.195465087890625\n",
      "Episode 780, Avg Reward: 15.93, PLoss: 13.787363052368164, VLoss: 13.155982971191406\n",
      "Episode 790, Avg Reward: 16.29, PLoss: 19.69816017150879, VLoss: 18.184383392333984\n",
      "Episode 800, Avg Reward: 16.81, PLoss: 36.29658889770508, VLoss: 34.21201705932617\n",
      "Episode 810, Avg Reward: 16.67, PLoss: 14.581123352050781, VLoss: 13.154192924499512\n",
      "Episode 820, Avg Reward: 17.07, PLoss: 9.736809730529785, VLoss: 9.164531707763672\n",
      "Episode 830, Avg Reward: 16.97, PLoss: 46.1777458190918, VLoss: 42.22747039794922\n",
      "Episode 840, Avg Reward: 17.01, PLoss: 19.6300106048584, VLoss: 18.15985107421875\n",
      "Episode 850, Avg Reward: 17.54, PLoss: 17.528743743896484, VLoss: 16.161325454711914\n",
      "Episode 860, Avg Reward: 17.42, PLoss: 21.986785888671875, VLoss: 20.198402404785156\n",
      "Episode 870, Avg Reward: 17.34, PLoss: 14.631400108337402, VLoss: 13.101588249206543\n",
      "Episode 880, Avg Reward: 17.27, PLoss: 27.040035247802734, VLoss: 25.19577980041504\n",
      "Episode 890, Avg Reward: 17.27, PLoss: 20.749980926513672, VLoss: 18.160587310791016\n",
      "Episode 900, Avg Reward: 16.88, PLoss: 9.523686408996582, VLoss: 9.169958114624023\n",
      "Episode 910, Avg Reward: 16.59, PLoss: 9.525689125061035, VLoss: 9.161797523498535\n",
      "Episode 920, Avg Reward: 16.19, PLoss: 21.346696853637695, VLoss: 19.19671058654785\n",
      "Episode 930, Avg Reward: 15.95, PLoss: 13.677457809448242, VLoss: 12.13625717163086\n",
      "Episode 940, Avg Reward: 16.1, PLoss: 10.209246635437012, VLoss: 9.169557571411133\n",
      "Episode 950, Avg Reward: 15.55, PLoss: 9.427227020263672, VLoss: 9.176815032958984\n",
      "Episode 960, Avg Reward: 15.72, PLoss: 28.225719451904297, VLoss: 26.19026756286621\n",
      "Episode 970, Avg Reward: 15.59, PLoss: 22.56545066833496, VLoss: 20.162940979003906\n",
      "Episode 980, Avg Reward: 15.69, PLoss: 22.99374771118164, VLoss: 21.191436767578125\n",
      "Episode 990, Avg Reward: 15.71, PLoss: 27.612506866455078, VLoss: 25.190998077392578\n",
      "Episode 1000, Avg Reward: 15.69, PLoss: 18.882827758789062, VLoss: 17.151399612426758\n",
      "Episode 1010, Avg Reward: 15.83, PLoss: 17.67686653137207, VLoss: 17.185930252075195\n",
      "Episode 1020, Avg Reward: 16.38, PLoss: 22.75782585144043, VLoss: 22.217613220214844\n",
      "Episode 1030, Avg Reward: 16.21, PLoss: 15.16108512878418, VLoss: 14.188398361206055\n",
      "Episode 1040, Avg Reward: 16.17, PLoss: 15.70331859588623, VLoss: 15.193547248840332\n",
      "Episode 1050, Avg Reward: 16.78, PLoss: 19.022737503051758, VLoss: 17.146623611450195\n",
      "Episode 1060, Avg Reward: 16.8, PLoss: 13.849509239196777, VLoss: 13.168472290039062\n",
      "Episode 1070, Avg Reward: 16.67, PLoss: 38.45900344848633, VLoss: 35.227298736572266\n",
      "Episode 1080, Avg Reward: 16.33, PLoss: 10.787348747253418, VLoss: 10.164202690124512\n",
      "Episode 1090, Avg Reward: 15.98, PLoss: 19.948780059814453, VLoss: 18.172286987304688\n",
      "Episode 1100, Avg Reward: 16.44, PLoss: 25.031902313232422, VLoss: 23.230382919311523\n",
      "Episode 1110, Avg Reward: 17.09, PLoss: 41.74100875854492, VLoss: 38.17247772216797\n",
      "Episode 1120, Avg Reward: 16.95, PLoss: 22.782108306884766, VLoss: 20.163145065307617\n",
      "Episode 1130, Avg Reward: 16.92, PLoss: 10.111154556274414, VLoss: 9.153153419494629\n",
      "Episode 1140, Avg Reward: 16.93, PLoss: 15.151955604553223, VLoss: 14.15591049194336\n",
      "Episode 1150, Avg Reward: 17.27, PLoss: 22.51581573486328, VLoss: 20.179466247558594\n",
      "Episode 1160, Avg Reward: 17.21, PLoss: 11.844362258911133, VLoss: 11.168212890625\n",
      "Episode 1170, Avg Reward: 17.09, PLoss: 20.34100914001465, VLoss: 19.216917037963867\n",
      "Episode 1180, Avg Reward: 17.53, PLoss: 23.41524887084961, VLoss: 22.2105770111084\n",
      "Episode 1190, Avg Reward: 17.55, PLoss: 14.59704875946045, VLoss: 13.174911499023438\n",
      "Episode 1200, Avg Reward: 16.76, PLoss: 18.548744201660156, VLoss: 17.183490753173828\n",
      "Episode 1210, Avg Reward: 16.02, PLoss: 17.959335327148438, VLoss: 16.17909812927246\n",
      "Episode 1220, Avg Reward: 16.03, PLoss: 22.302284240722656, VLoss: 20.17313003540039\n",
      "Episode 1230, Avg Reward: 16.48, PLoss: 34.25769805908203, VLoss: 31.178836822509766\n",
      "Episode 1240, Avg Reward: 16.64, PLoss: 13.145075798034668, VLoss: 12.178695678710938\n",
      "Episode 1250, Avg Reward: 16.02, PLoss: 15.323049545288086, VLoss: 13.165064811706543\n",
      "Episode 1260, Avg Reward: 16.16, PLoss: 15.366464614868164, VLoss: 14.172585487365723\n",
      "Episode 1270, Avg Reward: 16.15, PLoss: 17.583354949951172, VLoss: 16.153656005859375\n",
      "Episode 1280, Avg Reward: 15.58, PLoss: 9.828070640563965, VLoss: 9.152063369750977\n",
      "Episode 1290, Avg Reward: 15.97, PLoss: 11.83163070678711, VLoss: 10.165637969970703\n",
      "Episode 1300, Avg Reward: 16.6, PLoss: 22.642601013183594, VLoss: 21.195575714111328\n",
      "Episode 1310, Avg Reward: 17.26, PLoss: 23.25257682800293, VLoss: 21.180450439453125\n",
      "Episode 1320, Avg Reward: 17.12, PLoss: 19.860509872436523, VLoss: 18.1773738861084\n",
      "Episode 1330, Avg Reward: 16.75, PLoss: 12.933648109436035, VLoss: 12.134468078613281\n",
      "Episode 1340, Avg Reward: 17.09, PLoss: 17.02782440185547, VLoss: 15.165138244628906\n",
      "Episode 1350, Avg Reward: 17.12, PLoss: 24.635562896728516, VLoss: 22.1588077545166\n",
      "Episode 1360, Avg Reward: 16.8, PLoss: 14.066203117370605, VLoss: 13.185656547546387\n",
      "Episode 1370, Avg Reward: 17.35, PLoss: 14.306438446044922, VLoss: 13.15839958190918\n",
      "Episode 1380, Avg Reward: 17.71, PLoss: 16.63215446472168, VLoss: 15.165276527404785\n",
      "Episode 1390, Avg Reward: 17.81, PLoss: 15.042495727539062, VLoss: 13.17280387878418\n",
      "Episode 1400, Avg Reward: 17.95, PLoss: 20.756591796875, VLoss: 19.157615661621094\n",
      "Episode 1410, Avg Reward: 17.59, PLoss: 13.738090515136719, VLoss: 12.165327072143555\n",
      "Episode 1420, Avg Reward: 17.4, PLoss: 10.53631591796875, VLoss: 10.156609535217285\n",
      "Episode 1430, Avg Reward: 17.78, PLoss: 15.954358100891113, VLoss: 15.1636323928833\n",
      "Episode 1440, Avg Reward: 17.42, PLoss: 17.814298629760742, VLoss: 16.211103439331055\n",
      "Episode 1450, Avg Reward: 17.37, PLoss: 13.191648483276367, VLoss: 12.171262741088867\n",
      "Episode 1460, Avg Reward: 17.87, PLoss: 10.144015312194824, VLoss: 10.160280227661133\n",
      "Episode 1470, Avg Reward: 17.44, PLoss: 11.5366849899292, VLoss: 10.175973892211914\n",
      "Episode 1480, Avg Reward: 17.61, PLoss: 40.42377471923828, VLoss: 37.264610290527344\n",
      "Episode 1490, Avg Reward: 16.94, PLoss: 9.892365455627441, VLoss: 9.146296501159668\n",
      "Episode 1500, Avg Reward: 16.57, PLoss: 12.904684066772461, VLoss: 12.19097900390625\n",
      "Episode 1510, Avg Reward: 16.91, PLoss: 19.01908302307129, VLoss: 18.217464447021484\n",
      "Episode 1520, Avg Reward: 17.13, PLoss: 24.3631649017334, VLoss: 23.181171417236328\n",
      "Episode 1530, Avg Reward: 17.82, PLoss: 18.64913558959961, VLoss: 17.200672149658203\n",
      "Episode 1540, Avg Reward: 17.61, PLoss: 9.435307502746582, VLoss: 9.171319007873535\n",
      "Episode 1550, Avg Reward: 17.24, PLoss: 13.626392364501953, VLoss: 12.182147979736328\n",
      "Episode 1560, Avg Reward: 17.19, PLoss: 13.387112617492676, VLoss: 12.20650863647461\n",
      "Episode 1570, Avg Reward: 17.33, PLoss: 18.20754051208496, VLoss: 16.18255043029785\n",
      "Episode 1580, Avg Reward: 17.25, PLoss: 20.16580581665039, VLoss: 18.14934730529785\n",
      "Episode 1590, Avg Reward: 17.51, PLoss: 10.600530624389648, VLoss: 10.155226707458496\n",
      "Episode 1600, Avg Reward: 17.33, PLoss: 13.454436302185059, VLoss: 12.162775039672852\n",
      "Episode 1610, Avg Reward: 16.83, PLoss: 11.287372589111328, VLoss: 10.150619506835938\n",
      "Episode 1620, Avg Reward: 16.54, PLoss: 9.973592758178711, VLoss: 9.145061492919922\n",
      "Episode 1630, Avg Reward: 15.77, PLoss: 19.999074935913086, VLoss: 18.20136833190918\n",
      "Episode 1640, Avg Reward: 15.69, PLoss: 25.408836364746094, VLoss: 23.187597274780273\n",
      "Episode 1650, Avg Reward: 16.15, PLoss: 17.288761138916016, VLoss: 16.185644149780273\n",
      "Episode 1660, Avg Reward: 15.95, PLoss: 18.578170776367188, VLoss: 17.19016456604004\n",
      "Episode 1670, Avg Reward: 15.9, PLoss: 15.077811241149902, VLoss: 14.15482234954834\n",
      "Episode 1680, Avg Reward: 15.78, PLoss: 16.65447998046875, VLoss: 15.170343399047852\n",
      "Episode 1690, Avg Reward: 15.85, PLoss: 15.214421272277832, VLoss: 14.180861473083496\n",
      "Episode 1700, Avg Reward: 16.27, PLoss: 13.497605323791504, VLoss: 12.127898216247559\n",
      "Episode 1710, Avg Reward: 16.54, PLoss: 20.31989097595215, VLoss: 18.207820892333984\n",
      "Episode 1720, Avg Reward: 16.79, PLoss: 13.109436988830566, VLoss: 12.161120414733887\n",
      "Episode 1730, Avg Reward: 17.01, PLoss: 17.227924346923828, VLoss: 16.187992095947266\n",
      "Episode 1740, Avg Reward: 17.02, PLoss: 9.449563980102539, VLoss: 9.147115707397461\n",
      "Episode 1750, Avg Reward: 17.12, PLoss: 27.238752365112305, VLoss: 25.187631607055664\n",
      "Episode 1760, Avg Reward: 17.2, PLoss: 17.136362075805664, VLoss: 15.183902740478516\n",
      "Episode 1770, Avg Reward: 17.01, PLoss: 9.9386568069458, VLoss: 9.170783996582031\n",
      "Episode 1780, Avg Reward: 16.68, PLoss: 15.565526008605957, VLoss: 15.163248062133789\n",
      "Episode 1790, Avg Reward: 16.81, PLoss: 16.866796493530273, VLoss: 15.181581497192383\n",
      "Episode 1800, Avg Reward: 16.2, PLoss: 12.039117813110352, VLoss: 11.175237655639648\n",
      "Episode 1810, Avg Reward: 16.32, PLoss: 20.80668830871582, VLoss: 19.180587768554688\n",
      "Episode 1820, Avg Reward: 16.26, PLoss: 12.485085487365723, VLoss: 11.171671867370605\n",
      "Episode 1830, Avg Reward: 16.04, PLoss: 37.19120407104492, VLoss: 34.18691635131836\n",
      "Episode 1840, Avg Reward: 16.03, PLoss: 22.988540649414062, VLoss: 21.16067886352539\n",
      "Episode 1850, Avg Reward: 15.89, PLoss: 17.020401000976562, VLoss: 15.139744758605957\n",
      "Episode 1860, Avg Reward: 15.74, PLoss: 9.48443603515625, VLoss: 9.167272567749023\n",
      "Episode 1870, Avg Reward: 15.91, PLoss: 15.435110092163086, VLoss: 14.148917198181152\n",
      "Episode 1880, Avg Reward: 16.21, PLoss: 11.81273078918457, VLoss: 11.177518844604492\n",
      "Episode 1890, Avg Reward: 15.99, PLoss: 11.609058380126953, VLoss: 11.160746574401855\n",
      "Episode 1900, Avg Reward: 16.18, PLoss: 15.756736755371094, VLoss: 14.177507400512695\n",
      "Episode 1910, Avg Reward: 15.93, PLoss: 33.73361587524414, VLoss: 31.193378448486328\n",
      "Episode 1920, Avg Reward: 15.9, PLoss: 27.09285545349121, VLoss: 24.162572860717773\n",
      "Episode 1930, Avg Reward: 16.29, PLoss: 11.328217506408691, VLoss: 11.181354522705078\n",
      "Episode 1940, Avg Reward: 16.25, PLoss: 16.148643493652344, VLoss: 15.173394203186035\n",
      "Episode 1950, Avg Reward: 15.81, PLoss: 12.033980369567871, VLoss: 11.161140441894531\n",
      "Episode 1960, Avg Reward: 16.21, PLoss: 42.8897819519043, VLoss: 39.22602081298828\n",
      "Episode 1970, Avg Reward: 16.39, PLoss: 16.724275588989258, VLoss: 15.185750007629395\n",
      "Episode 1980, Avg Reward: 16.24, PLoss: 9.544574737548828, VLoss: 9.15156078338623\n",
      "Episode 1990, Avg Reward: 16.37, PLoss: 12.885529518127441, VLoss: 11.192649841308594\n"
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(agent.policy_network.state_dict(), 'models/ProgressiveCartPole_policy_network.pth')\n",
    "torch.save(agent.value_network.state_dict(), 'models/ProgressiveCartPole_value_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
