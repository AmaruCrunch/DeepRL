{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actorcritic import ActorCriticAgent, ContinuousActorCriticAgent, EnvironmentWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'experiment': 'MountainCarContinuous',\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 128, 64], \n",
    "    'lr_actor': 0.1,\n",
    "    'lr_critic': 0.1,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'MountainCarContinuous-v0',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 80.0,\n",
    "    'max_episodes': 1000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 250,\n",
    "    'discrete': False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def step(self, action):\n",
    "        # Assuming the first element of the padded action vector is the intended action for continuous spaces\n",
    "        action = action[0]\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "\n",
    "        # reward position and ignore penalty\n",
    "        position = state[0]\n",
    "        velocity = state[1]\n",
    "\n",
    "        reward = velocity * 100\n",
    "        \n",
    "        if position > 0.45:\n",
    "            reward += 100\n",
    "\n",
    "        return padded_state, reward, done, info\n",
    "\n",
    "    def action_padding(self, action):\n",
    "        # For continuous actions, it's already a float, pad to match target_action_size\n",
    "\n",
    "        padded_action = np.zeros(self.target_action_size)\n",
    "        padded_action[0] = action\n",
    "        return padded_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env)\n",
    "\n",
    "# Initialize the ActorCriticAgent\n",
    "agent = ContinuousActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -22.6329141954011, PLoss: -110.26020812988281, VLoss: 33.274932861328125\n",
      "Episode 10, Avg Reward: -5.037211619973774, PLoss: -24.021568298339844, VLoss: 46.67372512817383\n",
      "Episode 20, Avg Reward: -5.088498018106894, PLoss: -7.100100994110107, VLoss: 18.78367042541504\n",
      "Episode 30, Avg Reward: -2.9855371195729443, PLoss: -5.438972473144531, VLoss: 15.050861358642578\n",
      "Episode 40, Avg Reward: -5.486980249717849, PLoss: -58.036354064941406, VLoss: 117.58099365234375\n",
      "Episode 50, Avg Reward: -5.5531887732914464, PLoss: -62.6229133605957, VLoss: 32.085693359375\n",
      "Episode 60, Avg Reward: -4.4076327996115, PLoss: 89.00153350830078, VLoss: 68.31360626220703\n",
      "Episode 70, Avg Reward: -3.832739070852888, PLoss: 37.867523193359375, VLoss: 40.86731719970703\n",
      "Episode 80, Avg Reward: -3.4252720820430436, PLoss: 62.441566467285156, VLoss: 37.60742950439453\n",
      "Episode 90, Avg Reward: -2.972304682757764, PLoss: 38.61164474487305, VLoss: 6.947000503540039\n",
      "Episode 100, Avg Reward: -1.7475565594407243, PLoss: 30.968719482421875, VLoss: 11.134893417358398\n",
      "Episode 110, Avg Reward: -1.9726967471788583, PLoss: 0.09846830368041992, VLoss: 50.87178039550781\n",
      "Episode 120, Avg Reward: -0.6245687065119938, PLoss: 40.551570892333984, VLoss: 36.832496643066406\n",
      "Episode 130, Avg Reward: -1.6779245461888905, PLoss: -9.733671188354492, VLoss: 64.22504425048828\n",
      "Episode 140, Avg Reward: -0.749652647208535, PLoss: -20.378875732421875, VLoss: 7.45115852355957\n",
      "Episode 150, Avg Reward: -0.4789736057542484, PLoss: -3.359663486480713, VLoss: 51.12682342529297\n",
      "Episode 160, Avg Reward: -0.546585371391842, PLoss: 87.78763580322266, VLoss: 200.2631378173828\n",
      "Episode 170, Avg Reward: -1.5231719889712565, PLoss: -45.50782775878906, VLoss: 53.646514892578125\n",
      "Episode 180, Avg Reward: -1.3789197702065223, PLoss: -1.3430485725402832, VLoss: 58.17522430419922\n",
      "Episode 190, Avg Reward: -1.574396097109485, PLoss: -31.65452766418457, VLoss: 56.42975616455078\n",
      "Episode 200, Avg Reward: -1.9058283415795643, PLoss: -2.8303890228271484, VLoss: 35.60011672973633\n",
      "Episode 210, Avg Reward: -1.5122009141244064, PLoss: 36.625885009765625, VLoss: 19.232168197631836\n",
      "Episode 220, Avg Reward: -3.491663620158448, PLoss: -16.17123794555664, VLoss: 59.56069564819336\n",
      "Episode 230, Avg Reward: -2.3936144009177553, PLoss: 42.533992767333984, VLoss: 15.992207527160645\n",
      "Episode 240, Avg Reward: -2.316501484985565, PLoss: 72.40402221679688, VLoss: 73.80715942382812\n",
      "Episode 250, Avg Reward: -1.9590479745468201, PLoss: -23.307687759399414, VLoss: 66.27269744873047\n",
      "Episode 260, Avg Reward: -2.0435230485879856, PLoss: 43.30928421020508, VLoss: 47.09856414794922\n",
      "Episode 270, Avg Reward: -1.5418072928853235, PLoss: -81.28197479248047, VLoss: 41.741085052490234\n",
      "Episode 280, Avg Reward: -1.997587865647347, PLoss: 36.854888916015625, VLoss: 75.60894012451172\n",
      "Episode 290, Avg Reward: -1.7846908594193565, PLoss: -14.211633682250977, VLoss: 40.95600509643555\n",
      "Episode 300, Avg Reward: -2.811196455507911, PLoss: -45.985740661621094, VLoss: 184.50270080566406\n",
      "Episode 310, Avg Reward: -4.278299607880058, PLoss: -86.9467544555664, VLoss: 53.535587310791016\n",
      "Episode 320, Avg Reward: -3.9800698424284064, PLoss: 31.98700523376465, VLoss: 12.649686813354492\n",
      "Episode 330, Avg Reward: -4.834613902590604, PLoss: 49.32863998413086, VLoss: 25.235801696777344\n",
      "Episode 340, Avg Reward: -5.5693513220684565, PLoss: 80.50496673583984, VLoss: 77.94168090820312\n",
      "Episode 350, Avg Reward: -6.250478258696518, PLoss: -36.13129806518555, VLoss: 38.14640808105469\n",
      "Episode 360, Avg Reward: -6.621725346378668, PLoss: 98.69232940673828, VLoss: 88.17313385009766\n",
      "Episode 370, Avg Reward: -6.042752951807785, PLoss: -24.630022048950195, VLoss: 15.43049430847168\n",
      "Episode 380, Avg Reward: -6.13694662376875, PLoss: -27.719079971313477, VLoss: 19.31865119934082\n",
      "Episode 390, Avg Reward: -5.9232071045455825, PLoss: -54.20331954956055, VLoss: 29.054834365844727\n",
      "Episode 400, Avg Reward: -6.256402092330774, PLoss: 7.297554016113281, VLoss: 13.386112213134766\n",
      "Episode 410, Avg Reward: -4.05785020905185, PLoss: 90.39555358886719, VLoss: 81.86132049560547\n",
      "Episode 420, Avg Reward: -4.631252881337588, PLoss: 16.812808990478516, VLoss: 11.945393562316895\n",
      "Episode 430, Avg Reward: -4.289666871030505, PLoss: -13.570565223693848, VLoss: 60.54179382324219\n",
      "Episode 440, Avg Reward: -3.6190308614941387, PLoss: 84.52778625488281, VLoss: 138.4839630126953\n",
      "Episode 450, Avg Reward: -3.0017026714922395, PLoss: 13.147441864013672, VLoss: 136.6159210205078\n",
      "Episode 460, Avg Reward: -3.3362567482072336, PLoss: -44.37855529785156, VLoss: 175.7400360107422\n",
      "Episode 470, Avg Reward: -3.921726082623991, PLoss: -33.67351531982422, VLoss: 72.82951354980469\n",
      "Episode 480, Avg Reward: -4.8683017454777655, PLoss: -72.02487182617188, VLoss: 51.56721496582031\n",
      "Episode 490, Avg Reward: -5.8787925479582555, PLoss: -14.127212524414062, VLoss: 10.551244735717773\n",
      "Episode 500, Avg Reward: -5.393694506692611, PLoss: -12.336812973022461, VLoss: 76.21040344238281\n",
      "Episode 510, Avg Reward: -5.607058203894496, PLoss: -10.65542984008789, VLoss: 15.06643295288086\n",
      "Episode 520, Avg Reward: -5.24644384358254, PLoss: -119.85620880126953, VLoss: 485.9699401855469\n",
      "Episode 530, Avg Reward: -5.144803754314751, PLoss: 45.583858489990234, VLoss: 33.90873336791992\n",
      "Episode 540, Avg Reward: -5.811727268356833, PLoss: 32.74900817871094, VLoss: 34.6212043762207\n",
      "Episode 550, Avg Reward: -5.949568154991709, PLoss: 43.31813049316406, VLoss: 12.960713386535645\n",
      "Episode 560, Avg Reward: -4.6965191883584225, PLoss: -35.244991302490234, VLoss: 19.620790481567383\n",
      "Episode 570, Avg Reward: -5.400902629886048, PLoss: -63.056640625, VLoss: 73.40850067138672\n",
      "Episode 580, Avg Reward: -4.738948725987939, PLoss: -0.18945622444152832, VLoss: 20.31844139099121\n",
      "Episode 590, Avg Reward: -4.435312946504837, PLoss: 19.66914939880371, VLoss: 104.65933990478516\n",
      "Episode 600, Avg Reward: -5.735614767822824, PLoss: -104.26810455322266, VLoss: 145.73724365234375\n",
      "Episode 610, Avg Reward: -7.021457996009019, PLoss: -47.615875244140625, VLoss: 102.60871124267578\n",
      "Episode 620, Avg Reward: -6.687968687038563, PLoss: 13.388725280761719, VLoss: 93.62200927734375\n",
      "Episode 630, Avg Reward: -8.234327828526887, PLoss: -1.8234412670135498, VLoss: 24.32920265197754\n",
      "Episode 640, Avg Reward: -7.278238152608822, PLoss: -23.781299591064453, VLoss: 80.65889739990234\n",
      "Episode 650, Avg Reward: -8.119028352018915, PLoss: 28.971389770507812, VLoss: 52.166351318359375\n",
      "Episode 660, Avg Reward: -9.390763747457783, PLoss: 96.01441955566406, VLoss: 37.47806167602539\n",
      "Episode 670, Avg Reward: -8.682649968706215, PLoss: 65.91954040527344, VLoss: 53.680824279785156\n",
      "Episode 680, Avg Reward: -8.730452162410614, PLoss: -23.11317253112793, VLoss: 31.98069190979004\n",
      "Episode 690, Avg Reward: -8.497699674442714, PLoss: 7.809409141540527, VLoss: 17.53866195678711\n",
      "Episode 700, Avg Reward: -6.433966832208256, PLoss: 2.5485496520996094, VLoss: 56.29924392700195\n",
      "Episode 710, Avg Reward: -6.237590331776659, PLoss: -11.96607780456543, VLoss: 10.354498863220215\n",
      "Episode 720, Avg Reward: -6.354003383632147, PLoss: 22.502456665039062, VLoss: 46.30357360839844\n",
      "Episode 730, Avg Reward: -4.1920116335096225, PLoss: 19.441741943359375, VLoss: 43.579158782958984\n",
      "Episode 740, Avg Reward: -4.179961101563634, PLoss: -7.89154577255249, VLoss: 21.145339965820312\n",
      "Episode 750, Avg Reward: -3.531904359105141, PLoss: 29.337574005126953, VLoss: 33.3739128112793\n",
      "Episode 760, Avg Reward: -3.43113194010175, PLoss: -87.55787658691406, VLoss: 200.95904541015625\n",
      "Episode 770, Avg Reward: -3.841919647961504, PLoss: -42.72047424316406, VLoss: 61.01304626464844\n",
      "Episode 780, Avg Reward: -3.3366209659059223, PLoss: -33.0549201965332, VLoss: 46.044490814208984\n",
      "Episode 790, Avg Reward: -3.2912478814989754, PLoss: 48.81803512573242, VLoss: 43.30677795410156\n",
      "Episode 800, Avg Reward: -5.30594552520256, PLoss: 23.73597526550293, VLoss: 56.444908142089844\n",
      "Episode 810, Avg Reward: -5.7190644882499555, PLoss: -59.788204193115234, VLoss: 37.29277801513672\n",
      "Episode 820, Avg Reward: -5.224969601809335, PLoss: 11.903883934020996, VLoss: 23.586830139160156\n",
      "Episode 830, Avg Reward: -6.178498746232435, PLoss: 64.05224609375, VLoss: 120.75830078125\n",
      "Episode 840, Avg Reward: -5.82964481894669, PLoss: 65.44832611083984, VLoss: 28.614046096801758\n",
      "Episode 850, Avg Reward: -5.765568700643598, PLoss: 18.85120964050293, VLoss: 10.885404586791992\n",
      "Episode 860, Avg Reward: -5.772761096215845, PLoss: -51.31385803222656, VLoss: 54.24694061279297\n",
      "Episode 870, Avg Reward: -5.695701909862919, PLoss: 21.351375579833984, VLoss: 26.400007247924805\n",
      "Episode 880, Avg Reward: -5.089219163643747, PLoss: -48.951515197753906, VLoss: 17.667999267578125\n",
      "Episode 890, Avg Reward: -6.144238179268996, PLoss: -51.79417419433594, VLoss: 41.170413970947266\n",
      "Episode 900, Avg Reward: -4.54079035055242, PLoss: 38.864017486572266, VLoss: 46.3232307434082\n",
      "Episode 910, Avg Reward: -3.3570504827839964, PLoss: 130.38978576660156, VLoss: 75.42337036132812\n",
      "Episode 920, Avg Reward: -3.7360694908117362, PLoss: -15.847049713134766, VLoss: 11.380395889282227\n",
      "Episode 930, Avg Reward: -4.057065716211959, PLoss: 17.365137100219727, VLoss: 146.39794921875\n",
      "Episode 940, Avg Reward: -4.617952131642063, PLoss: -21.965431213378906, VLoss: 14.403992652893066\n",
      "Episode 950, Avg Reward: -5.316797543833367, PLoss: -45.71812438964844, VLoss: 72.42687225341797\n",
      "Episode 960, Avg Reward: -5.17970484292065, PLoss: -49.211666107177734, VLoss: 14.756731033325195\n",
      "Episode 970, Avg Reward: -5.131255939571396, PLoss: 3.115133762359619, VLoss: 16.477373123168945\n",
      "Episode 980, Avg Reward: -5.60163776506662, PLoss: 34.70551300048828, VLoss: 23.02517318725586\n",
      "Episode 990, Avg Reward: -5.449387597428519, PLoss: 92.5732421875, VLoss: 188.84095764160156\n"
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=1000, max_steps=200, reward_threshold=200, update_frequency=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def step(self, action):\n",
    "        # Assuming the first element of the padded action vector is the intended action for continuous spaces\n",
    "        action = action[0]\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "\n",
    "        return padded_state, reward, done, info\n",
    "\n",
    "    def action_padding(self, action):\n",
    "        # For continuous actions, it's already a float, pad to match target_action_size\n",
    "\n",
    "        padded_action = np.zeros(self.target_action_size)\n",
    "        padded_action[0] = action\n",
    "        return padded_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -52.755937436186905, PLoss: -85.64596557617188, VLoss: 6.484879016876221\n",
      "Episode 10, Avg Reward: -51.03031835682229, PLoss: -82.35324096679688, VLoss: 5.460649013519287\n",
      "Episode 20, Avg Reward: -50.628964224693036, PLoss: -92.96109008789062, VLoss: 6.803727626800537\n",
      "Episode 30, Avg Reward: -50.16470561445307, PLoss: -80.71420288085938, VLoss: 5.9874653816223145\n",
      "Episode 40, Avg Reward: -50.24910531623818, PLoss: -52.20458984375, VLoss: 4.2186174392700195\n",
      "Episode 50, Avg Reward: -50.3915127874746, PLoss: -65.89071655273438, VLoss: 5.160287857055664\n",
      "Episode 60, Avg Reward: -49.99458309489909, PLoss: -34.0566291809082, VLoss: 4.391839981079102\n",
      "Episode 70, Avg Reward: -50.03684971205408, PLoss: -81.07273864746094, VLoss: 9.297908782958984\n",
      "Episode 80, Avg Reward: -50.153439642885985, PLoss: -53.1116828918457, VLoss: 6.417478561401367\n",
      "Episode 90, Avg Reward: -50.13915479508059, PLoss: -24.704288482666016, VLoss: 4.856125354766846\n",
      "Episode 100, Avg Reward: -50.0376482431558, PLoss: -34.717567443847656, VLoss: 6.639535903930664\n",
      "Episode 110, Avg Reward: -49.89918675962079, PLoss: -19.290258407592773, VLoss: 5.431020259857178\n",
      "Episode 120, Avg Reward: -49.834873753038764, PLoss: -31.656705856323242, VLoss: 6.245692253112793\n",
      "Episode 130, Avg Reward: -49.84897916368125, PLoss: -20.56673240661621, VLoss: 4.166560173034668\n",
      "Episode 140, Avg Reward: -49.7315715858703, PLoss: -28.121551513671875, VLoss: 5.23110818862915\n",
      "Episode 150, Avg Reward: -49.579201793959164, PLoss: -26.532419204711914, VLoss: 4.795707702636719\n",
      "Episode 160, Avg Reward: -49.737434940010665, PLoss: -46.33878707885742, VLoss: 6.162044525146484\n",
      "Episode 170, Avg Reward: -49.69071905217522, PLoss: -30.31568717956543, VLoss: 4.697380542755127\n",
      "Episode 180, Avg Reward: -49.65597371781213, PLoss: -61.78056335449219, VLoss: 8.041179656982422\n",
      "Episode 190, Avg Reward: -49.51570782874848, PLoss: -29.24672508239746, VLoss: 4.956888198852539\n",
      "Episode 200, Avg Reward: -49.68280925429206, PLoss: -13.365426063537598, VLoss: 4.4169816970825195\n",
      "Episode 210, Avg Reward: -49.70944562657013, PLoss: -66.19124603271484, VLoss: 9.185928344726562\n",
      "Episode 220, Avg Reward: -49.72451764252644, PLoss: -12.346858978271484, VLoss: 3.6765317916870117\n",
      "Episode 230, Avg Reward: -49.9908539353807, PLoss: -14.786259651184082, VLoss: 4.619802951812744\n",
      "Episode 240, Avg Reward: -50.11508695701907, PLoss: -28.306230545043945, VLoss: 5.314778804779053\n",
      "Episode 250, Avg Reward: -50.17329301542879, PLoss: -13.122932434082031, VLoss: 4.5124359130859375\n",
      "Episode 260, Avg Reward: -50.20650736740622, PLoss: -9.445478439331055, VLoss: 4.059823513031006\n",
      "Episode 270, Avg Reward: -50.34488054919586, PLoss: -43.35425567626953, VLoss: 5.370823860168457\n",
      "Episode 280, Avg Reward: -50.35344644779847, PLoss: -6.681767463684082, VLoss: 3.9958066940307617\n",
      "Episode 290, Avg Reward: -50.57181162685992, PLoss: -26.523099899291992, VLoss: 4.977667808532715\n",
      "Episode 300, Avg Reward: -49.36621550642293, PLoss: -25.31633186340332, VLoss: 4.832516193389893\n",
      "Episode 310, Avg Reward: -49.38309629699516, PLoss: -36.758296966552734, VLoss: 4.764254570007324\n",
      "Episode 320, Avg Reward: -49.43021836017843, PLoss: -51.206871032714844, VLoss: 7.654275894165039\n",
      "Episode 330, Avg Reward: -49.30972644786394, PLoss: -8.167448997497559, VLoss: 3.1876182556152344\n",
      "Episode 340, Avg Reward: -49.291854855177796, PLoss: -22.40909194946289, VLoss: 4.483830451965332\n",
      "Episode 350, Avg Reward: -49.33050608755534, PLoss: -35.98446273803711, VLoss: 5.309427261352539\n",
      "Episode 360, Avg Reward: -49.40106143390729, PLoss: -20.399139404296875, VLoss: 3.88451886177063\n",
      "Episode 370, Avg Reward: -49.32196956723513, PLoss: -21.4542179107666, VLoss: 4.482114315032959\n",
      "Episode 380, Avg Reward: -49.194664638201196, PLoss: -22.06243324279785, VLoss: 5.523955345153809\n",
      "Episode 390, Avg Reward: -49.20310978265361, PLoss: -21.451189041137695, VLoss: 4.890266418457031\n",
      "Episode 400, Avg Reward: -50.186888813423984, PLoss: -19.87070655822754, VLoss: 4.607981204986572\n",
      "Episode 410, Avg Reward: -50.11778809696492, PLoss: -13.227119445800781, VLoss: 3.8337502479553223\n",
      "Episode 420, Avg Reward: -50.21092166920758, PLoss: -29.665393829345703, VLoss: 5.738188743591309\n",
      "Episode 430, Avg Reward: -50.11913072670625, PLoss: 3.72031569480896, VLoss: 3.052762269973755\n",
      "Episode 440, Avg Reward: -50.19731199704868, PLoss: -28.648954391479492, VLoss: 4.772961139678955\n"
     ]
    }
   ],
   "source": [
    "env_wrapper = MCCWrapper(env)\n",
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results with model name and time\n",
    "import datetime\n",
    "import numpy as np\n",
    "now = datetime.datetime.now()\n",
    "np.save(f'results/{config[\"env_name\"]}_{now.strftime(\"%Y-%m-%d_%H-%M\")}.csv', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
