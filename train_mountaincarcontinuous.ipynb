{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actorcritic import ActorCriticAgent, EnvironmentWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 128, 128, 64], \n",
    "    'lr_actor': 0.001,\n",
    "    'lr_critic': 0.005,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'MountainCarContinuous-v0',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 90.0,\n",
    "    'max_episodes': 1000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 250\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def __init__(self, env, target_state_size=6, target_action_size=3, optimize_action=True, encourage=False):\n",
    "        super().__init__(env, target_state_size, target_action_size)\n",
    "        self.optimize_action = optimize_action\n",
    "        self.encourage = encourage\n",
    "        self.last_action = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Assuming the first element of the padded action vector is the intended action for continuous spaces\n",
    "        action = np.array([action])\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "\n",
    "        if self.encourage:      \n",
    "            position = state[0]\n",
    "            velocity = state[1]\n",
    "            if position > 0.05:\n",
    "                reward += 50 # Encourage both position and velocity\n",
    "\n",
    "                if self.last_action * action[0] > 0:\n",
    "                    reward += 0.1\n",
    "\n",
    "        if not self.optimize_action:\n",
    "            # Remove action penalty during initial episodes\n",
    "            action_penalty = 0.01 * action[0]**2\n",
    "            reward += action_penalty  # Add back the subtracted penalty to neutralize it\n",
    "\n",
    "        return padded_state, reward, done, info\n",
    "\n",
    "    def action_padding(self, action):\n",
    "        # For continuous actions, it's already a float, pad to match target_action_size\n",
    "\n",
    "        padded_action = np.zeros(self.target_action_size)\n",
    "        padded_action[0] = action\n",
    "        return padded_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env, optimize_action=False, encourage=True)\n",
    "\n",
    "# Initialize the ActorCriticAgent\n",
    "agent = ActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -25.469999999999974, PLoss: -18.23121452331543, VLoss: 4.76250696182251\n",
      "Episode 10, Avg Reward: -27.34363636363633, PLoss: -14.18409252166748, VLoss: 3.2114710807800293\n",
      "Episode 20, Avg Reward: -23.575714285714266, PLoss: -5.386652946472168, VLoss: 1.8356647491455078\n",
      "Episode 30, Avg Reward: -18.34838709677418, PLoss: -3.932079553604126, VLoss: 0.24220699071884155\n",
      "Episode 40, Avg Reward: -14.015853658536574, PLoss: -0.4094262719154358, VLoss: 0.011798807419836521\n",
      "Episode 50, Avg Reward: -11.272941176470578, PLoss: 0.00010030555131379515, VLoss: 0.0003406023024581373\n",
      "Episode 60, Avg Reward: -9.424918032786877, PLoss: 5.211324605625123e-06, VLoss: 1.3916159332438838e-05\n",
      "Episode 70, Avg Reward: -8.097464788732387, PLoss: -1.2873040304839378e-06, VLoss: 2.8735000796586974e-06\n",
      "Episode 80, Avg Reward: -7.097777777777772, PLoss: 5.855249582964461e-07, VLoss: 9.845788326856564e-07\n",
      "Episode 90, Avg Reward: -6.317802197802193, PLoss: -2.3283814698515926e-07, VLoss: 1.9685857921558636e-07\n",
      "Episode 100, Avg Reward: -5.494499999999996, PLoss: 9.9888815441318e-08, VLoss: 3.7970899313677364e-08\n",
      "Episode 110, Avg Reward: -2.741399999999997, PLoss: 3.7149201492781003e-09, VLoss: 1.7080940828417823e-10\n",
      "Episode 120, Avg Reward: -0.7982999999999997, PLoss: -3.14869375017679e-08, VLoss: 4.7373585054799605e-09\n",
      "Episode 130, Avg Reward: -0.06119999999999999, PLoss: 3.6316716744977384e-08, VLoss: 5.290059057472263e-09\n",
      "Episode 140, Avg Reward: -0.0027, PLoss: -2.032885859648559e-08, VLoss: 1.8380034161324943e-09\n",
      "Episode 150, Avg Reward: -0.0009000000000000001, PLoss: 4.7089821464396664e-07, VLoss: 1.1199171012776787e-06\n",
      "Episode 160, Avg Reward: -0.0009000000000000001, PLoss: 3.752972475012939e-07, VLoss: 1.483794221712742e-06\n",
      "Episode 170, Avg Reward: -0.0009000000000000001, PLoss: -2.1614549439163966e-07, VLoss: 6.445931148846284e-07\n",
      "Episode 180, Avg Reward: -0.0009000000000000001, PLoss: 1.3117511343807564e-07, VLoss: 2.5957888283301145e-07\n",
      "Episode 190, Avg Reward: -0.0009000000000000001, PLoss: -7.461192552682405e-08, VLoss: 8.843071697128835e-08\n",
      "Episode 200, Avg Reward: -0.0009000000000000001, PLoss: 4.297996625268752e-08, VLoss: 2.9289569880575073e-08\n",
      "Episode 210, Avg Reward: -0.0009000000000000001, PLoss: -2.364980389302218e-08, VLoss: 8.812421903314771e-09\n",
      "Episode 220, Avg Reward: -0.0009000000000000001, PLoss: 1.4123028080348377e-08, VLoss: 3.4756739708541318e-09\n",
      "Episode 230, Avg Reward: -0.0009000000000000001, PLoss: -7.398220702725666e-09, VLoss: 1.2625899215024106e-09\n",
      "Episode 240, Avg Reward: -0.0009000000000000001, PLoss: 4.9884421038370874e-09, VLoss: 4.758047844610758e-10\n",
      "Episode 250, Avg Reward: 0.0, PLoss: -2.567525303476259e-09, VLoss: 1.514707526073522e-10\n",
      "Episode 260, Avg Reward: 0.0, PLoss: 1.8284597169682115e-09, VLoss: 6.79570566486376e-11\n",
      "Episode 270, Avg Reward: 0.0, PLoss: -6.369476612810843e-10, VLoss: 2.3503653884260345e-11\n",
      "Episode 280, Avg Reward: 0.0, PLoss: 8.140564333736222e-10, VLoss: 1.4347317604801457e-11\n",
      "Episode 290, Avg Reward: 0.0, PLoss: -3.6416664128680054e-10, VLoss: 5.927034973929324e-10\n",
      "Episode 300, Avg Reward: 0.0, PLoss: 6.978372879551387e-10, VLoss: 1.589426854020637e-11\n",
      "Episode 310, Avg Reward: 0.0, PLoss: 1.0734064331829885e-10, VLoss: 1.34494256009976e-11\n",
      "Episode 320, Avg Reward: 0.0, PLoss: 4.1739539602403397e-10, VLoss: 3.1892719801890346e-12\n",
      "Episode 330, Avg Reward: 0.0, PLoss: 1.798233506544733e-10, VLoss: 5.500069800642993e-13\n",
      "Episode 340, Avg Reward: 0.0, PLoss: 3.610469700987551e-10, VLoss: 1.3894391713564769e-11\n",
      "Episode 350, Avg Reward: 0.0, PLoss: 4.941965725535624e-10, VLoss: 3.2318057952007706e-11\n",
      "Episode 360, Avg Reward: 0.0, PLoss: 1.6544263181650365e-10, VLoss: 6.044679201622216e-10\n",
      "Episode 370, Avg Reward: -0.0009000000000000001, PLoss: 2.607922340303048e-07, VLoss: 3.519731080814381e-06\n",
      "Episode 380, Avg Reward: -0.0009000000000000001, PLoss: -2.335412574439033e-08, VLoss: 7.087579234621444e-08\n",
      "Episode 390, Avg Reward: -0.0009000000000000001, PLoss: 2.6303970557250977e-08, VLoss: 1.1250065767853812e-07\n",
      "Episode 400, Avg Reward: -0.0009000000000000001, PLoss: -4.039638845654281e-09, VLoss: 3.062364584138777e-09\n",
      "Episode 410, Avg Reward: -0.0009000000000000001, PLoss: 4.140391585139014e-09, VLoss: 3.4224594269716135e-09\n",
      "Episode 420, Avg Reward: -0.0009000000000000001, PLoss: -1.1552221401700535e-09, VLoss: 2.852374170192462e-10\n",
      "Episode 430, Avg Reward: -0.0009000000000000001, PLoss: 1.2465437571052007e-09, VLoss: 3.4729794040622153e-10\n",
      "Episode 440, Avg Reward: -0.0009000000000000001, PLoss: -6.066102065105383e-10, VLoss: 1.8800404288921158e-10\n",
      "Episode 450, Avg Reward: -0.0009000000000000001, PLoss: 6.914718797546016e-10, VLoss: 5.776940592561175e-10\n",
      "Episode 460, Avg Reward: -0.0009000000000000001, PLoss: -2.6800051067255026e-10, VLoss: 1.9788717886548568e-10\n",
      "Episode 470, Avg Reward: 0.0, PLoss: 3.0631575054229643e-10, VLoss: 6.817844899753567e-11\n",
      "Episode 480, Avg Reward: 0.0, PLoss: -1.9132471995586542e-10, VLoss: 9.273158629863332e-12\n",
      "Episode 490, Avg Reward: 0.0, PLoss: 2.560305578658273e-10, VLoss: 4.645022699811818e-10\n",
      "Episode 500, Avg Reward: 0.0, PLoss: -5.6302067175106885e-11, VLoss: 7.741595992732941e-13\n",
      "Episode 510, Avg Reward: 0.0, PLoss: 1.8878716645520655e-10, VLoss: 2.6979182776720734e-11\n",
      "Episode 520, Avg Reward: 0.0, PLoss: -1.0039840486752993e-10, VLoss: 1.639584024104579e-10\n",
      "Episode 530, Avg Reward: 0.0, PLoss: 2.18540546770285e-10, VLoss: 3.292385114539442e-11\n",
      "Episode 540, Avg Reward: 0.0, PLoss: -3.043361049281934e-11, VLoss: 1.239513908180001e-10\n",
      "Episode 550, Avg Reward: 0.0, PLoss: 9.116355587091007e-11, VLoss: 1.8936907163896732e-12\n",
      "Episode 560, Avg Reward: 0.0, PLoss: 6.282724340778145e-11, VLoss: 4.544068454848116e-10\n",
      "Episode 570, Avg Reward: 0.0, PLoss: 1.1212605843802237e-10, VLoss: 6.325871493295665e-10\n",
      "Episode 580, Avg Reward: 0.0, PLoss: 5.607435349386236e-11, VLoss: 2.0685740446946266e-12\n",
      "Episode 590, Avg Reward: 0.0, PLoss: 4.2350980361538504e-11, VLoss: 9.346477758409577e-11\n",
      "Episode 600, Avg Reward: 0.0, PLoss: 1.7448657796403921e-10, VLoss: 4.134173836600752e-10\n",
      "Episode 610, Avg Reward: 0.0, PLoss: 9.729882871623019e-11, VLoss: 5.882862946782197e-11\n",
      "Episode 620, Avg Reward: 0.0, PLoss: 1.0429792446364772e-10, VLoss: 7.558285594622127e-12\n",
      "Episode 630, Avg Reward: 0.0, PLoss: 1.1441030761671911e-10, VLoss: 5.454114937464283e-11\n",
      "Episode 640, Avg Reward: 0.0, PLoss: 1.7225586235181112e-10, VLoss: 4.421180782587797e-11\n",
      "Episode 650, Avg Reward: 0.0, PLoss: 1.4308727813716615e-11, VLoss: 3.898494582710299e-10\n",
      "Episode 660, Avg Reward: 0.0, PLoss: 1.2405806937287878e-10, VLoss: 8.821159948324553e-12\n",
      "Episode 670, Avg Reward: 0.0, PLoss: 7.83459616626736e-11, VLoss: 1.908448260534712e-10\n",
      "Episode 680, Avg Reward: 0.0, PLoss: 2.856186814836903e-11, VLoss: 6.156873344487224e-10\n",
      "Episode 690, Avg Reward: 0.0, PLoss: 1.4801510017647956e-10, VLoss: 1.7999556012338047e-10\n",
      "Episode 700, Avg Reward: 0.0, PLoss: 7.253118938788106e-11, VLoss: 4.268112339250729e-12\n",
      "Episode 710, Avg Reward: 0.0, PLoss: 1.1997253190898505e-10, VLoss: 2.4201446402472016e-10\n",
      "Episode 720, Avg Reward: 0.0, PLoss: 1.5226343796914676e-10, VLoss: 1.7829317883188978e-11\n",
      "Episode 730, Avg Reward: 0.0, PLoss: -3.0528674206664608e-12, VLoss: 6.414109261082546e-12\n",
      "Episode 740, Avg Reward: 0.0, PLoss: 1.521004572291318e-10, VLoss: 1.4545403770327425e-10\n",
      "Episode 750, Avg Reward: 0.0, PLoss: 7.683899350130474e-11, VLoss: 3.373738788337022e-11\n",
      "Episode 760, Avg Reward: 0.0, PLoss: 3.5982879870166684e-11, VLoss: 1.838338925530536e-10\n",
      "Episode 770, Avg Reward: 0.0, PLoss: 1.570350377511076e-10, VLoss: 1.4365354392142926e-11\n",
      "Episode 780, Avg Reward: 0.0, PLoss: 1.033760091417868e-10, VLoss: 1.6716958373130808e-10\n",
      "Episode 790, Avg Reward: 0.0, PLoss: 1.4634071732189113e-10, VLoss: 2.5461671659954277e-10\n",
      "Episode 800, Avg Reward: 0.0, PLoss: 1.432805679657534e-10, VLoss: 8.092802122883214e-11\n",
      "Episode 810, Avg Reward: 0.0, PLoss: 2.92937583912245e-11, VLoss: 7.2582564958345586e-12\n",
      "Episode 820, Avg Reward: 0.0, PLoss: 1.5838851064042814e-10, VLoss: 8.210630092486682e-12\n",
      "Episode 830, Avg Reward: 0.0, PLoss: 1.5472059744503497e-10, VLoss: 3.049255015152852e-10\n",
      "Episode 840, Avg Reward: 0.0, PLoss: -5.659000004709647e-11, VLoss: 6.573839237723755e-13\n",
      "Episode 850, Avg Reward: 0.0, PLoss: 1.8018826708487978e-10, VLoss: 1.4461559554035386e-11\n",
      "Episode 860, Avg Reward: 0.0, PLoss: 5.693772883730297e-11, VLoss: 2.520115409757051e-12\n",
      "Episode 870, Avg Reward: 0.0, PLoss: 1.971192480076933e-11, VLoss: 6.069478253323268e-10\n",
      "Episode 880, Avg Reward: 0.0, PLoss: 1.0792445409579798e-10, VLoss: 3.910730420053632e-11\n",
      "Episode 890, Avg Reward: 0.0, PLoss: 1.729283799489778e-10, VLoss: 3.398660797238051e-10\n",
      "Episode 900, Avg Reward: 0.0, PLoss: -2.5682161258755443e-11, VLoss: 4.783926588203258e-10\n",
      "Episode 910, Avg Reward: 0.0, PLoss: 7.188381140332822e-11, VLoss: 4.6796656827385874e-11\n",
      "Episode 920, Avg Reward: 0.0, PLoss: 1.2553516559599132e-10, VLoss: 3.07028208412774e-10\n"
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=1000, max_steps=200, reward_threshold=200, update_frequency=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 0.0, PLoss: -8.791076200331815e-10, VLoss: 3.3721200054515066e-08\n",
      "Episode 10, Avg Reward: 0.0, PLoss: -3.956101224389386e-09, VLoss: 6.689766962608701e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env_wrapper \u001b[38;5;241m=\u001b[39m MCCWrapper(env, optimize_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encourage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m124\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:167\u001b[0m, in \u001b[0;36mActorCriticAgent.train\u001b[0;34m(self, env_wrapper, max_episodes, max_steps, reward_threshold, update_frequency)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# update policy\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transitions) \u001b[38;5;241m>\u001b[39m update_frequency \u001b[38;5;129;01mor\u001b[39;00m done:\n\u001b[0;32m--> 167\u001b[0m     loss_policy, loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_policy)\n\u001b[1;32m    169\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:118\u001b[0m, in \u001b[0;36mActorCriticAgent.update_policy\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    115\u001b[0m loss_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(predicted_value, expected_value\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Compute policy loss\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[1;32m    120\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:29\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/modules/linear.py:94\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/nn/functional.py:1753\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_wrapper = MCCWrapper(env, optimize_action=False, encourage=False)\n",
    "results = agent.train(env_wrapper, max_episodes=1000, max_steps=200, reward_threshold=200, update_frequency=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 0.0, PLoss: -0.00014008763537276536, VLoss: 39.984580993652344\n",
      "Episode 10, Avg Reward: 0.0, PLoss: -1.1904383256933215e-07, VLoss: 3.04175446217414e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env_wrapper \u001b[38;5;241m=\u001b[39m MCCWrapper(env, optimize_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encourage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward_threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:167\u001b[0m, in \u001b[0;36mActorCriticAgent.train\u001b[0;34m(self, env_wrapper, max_episodes, max_steps, reward_threshold, update_frequency)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# update policy\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transitions) \u001b[38;5;241m>\u001b[39m update_frequency \u001b[38;5;129;01mor\u001b[39;00m done:\n\u001b[0;32m--> 167\u001b[0m     loss_policy, loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_policy)\n\u001b[1;32m    169\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:105\u001b[0m, in \u001b[0;36mActorCriticAgent.update_policy\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m transitions:\n\u001b[1;32m    104\u001b[0m     state, action, reward, next_state, done \u001b[38;5;241m=\u001b[39m transition\n\u001b[0;32m--> 105\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    107\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([action])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_wrapper = MCCWrapper(env, optimize_action=False, encourage=False)\n",
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results with model name and time\n",
    "import datetime\n",
    "import numpy as np\n",
    "now = datetime.datetime.now()\n",
    "np.save(f'results/{config[\"env_name\"]}_{now.strftime(\"%Y-%m-%d_%H-%M\")}.csv', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
