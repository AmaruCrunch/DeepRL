{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actorcritic import ActorCriticAgent, EnvironmentWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'experiment': 'MountainCarContinuous',\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 64], \n",
    "    'lr_actor': 0.001,\n",
    "    'lr_critic': 0.0005,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'MountainCarContinuous-v0',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 80.0,\n",
    "    'max_episodes': 5000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 500,\n",
    "    'discrete': True\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def __init__(self, env, num_actions=5):\n",
    "        super().__init__(env)\n",
    "        self.action_space = num_actions\n",
    "        # Define the discrete action boundaries\n",
    "        self.action_boundaries = np.linspace(-1, 1, num_actions)\n",
    "        self.ticker = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert the discrete action into a continuous action\n",
    "        continuous_action = [self.discretize_action(action)]\n",
    "\n",
    "\n",
    "        # Step using the continuous action\n",
    "        state, reward, done, _, info = self.env.step(continuous_action)\n",
    "\n",
    "        if self.ticker < 1000:\n",
    "            reward = state[0] * state[1] * 0.1 # moving up-hill is good\n",
    "            \n",
    "            if state[0] >= 0.45:\n",
    "                reward += 100\n",
    "        \n",
    "\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "        \n",
    "        self.ticker += 1\n",
    "\n",
    "        return padded_state, reward, done, info\n",
    "    \n",
    "\n",
    "\n",
    "    def discretize_action(self, action):\n",
    "        # Ensure the action is within the valid range\n",
    "        action = max(0, min(action, self.action_space - 1))\n",
    "        return self.action_boundaries[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env, num_actions=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ActorCriticAgent\n",
    "agent = ActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 0.005652161398234069, PLoss: 0.051719777286052704, VLoss: 0.003804302541539073\n",
      "Episode 10, Avg Reward: -29.589759890698904, PLoss: -54.414520263671875, VLoss: 4.941513538360596\n",
      "Episode 20, Avg Reward: -29.11844565703296, PLoss: 60.2781982421875, VLoss: 10005.21875\n",
      "Episode 30, Avg Reward: -32.05120512250634, PLoss: -53.95663833618164, VLoss: 4.829858303070068\n",
      "Episode 40, Avg Reward: -36.428959970675635, PLoss: -53.58173751831055, VLoss: 4.768331050872803\n",
      "Episode 50, Avg Reward: -39.089948211719715, PLoss: -53.16948699951172, VLoss: 4.700223922729492\n",
      "Episode 60, Avg Reward: -39.17684194750343, PLoss: -52.72344207763672, VLoss: 4.6161065101623535\n",
      "Episode 70, Avg Reward: -40.701230405601606, PLoss: -52.12405014038086, VLoss: 4.530447483062744\n",
      "Episode 80, Avg Reward: -40.556634059231094, PLoss: -51.46949005126953, VLoss: 4.459988594055176\n",
      "Episode 90, Avg Reward: -41.59436658019476, PLoss: -50.844058990478516, VLoss: 4.277740478515625\n",
      "Episode 100, Avg Reward: -41.730930109591256, PLoss: -49.25185012817383, VLoss: 4.151313781738281\n",
      "Episode 110, Avg Reward: -42.28900000000042, PLoss: -48.2699089050293, VLoss: 3.9371674060821533\n",
      "Episode 120, Avg Reward: -42.28600000000042, PLoss: -47.90633773803711, VLoss: 3.867044448852539\n",
      "Episode 130, Avg Reward: -41.11900000000043, PLoss: 68.79786682128906, VLoss: 10132.8095703125\n",
      "Episode 140, Avg Reward: -41.11900000000043, PLoss: -43.83633041381836, VLoss: 3.4359068870544434\n",
      "Episode 150, Avg Reward: -41.11900000000043, PLoss: -42.61283874511719, VLoss: 3.672943115234375\n",
      "Episode 160, Avg Reward: -39.80200000000043, PLoss: -41.046695709228516, VLoss: 3.0230956077575684\n",
      "Episode 170, Avg Reward: -39.80200000000043, PLoss: -39.226253509521484, VLoss: 2.9299745559692383\n",
      "Episode 180, Avg Reward: -39.81000000000043, PLoss: -36.29560852050781, VLoss: 2.332205057144165\n",
      "Episode 190, Avg Reward: -39.81000000000043, PLoss: -32.7385368347168, VLoss: 3.1876089572906494\n",
      "Episode 200, Avg Reward: -39.79100000000044, PLoss: -30.68736457824707, VLoss: 2.231895923614502\n",
      "Episode 210, Avg Reward: -36.42600000000043, PLoss: -28.132152557373047, VLoss: 2.1131083965301514\n",
      "Episode 220, Avg Reward: -38.56900000000043, PLoss: -27.66431999206543, VLoss: 2.1307332515716553\n",
      "Episode 230, Avg Reward: -40.91500000000043, PLoss: -24.557327270507812, VLoss: 1.8321232795715332\n",
      "Episode 240, Avg Reward: -39.70100000000043, PLoss: -21.545774459838867, VLoss: 2.086329936981201\n",
      "Episode 250, Avg Reward: -39.70100000000043, PLoss: -19.053123474121094, VLoss: 2.0009143352508545\n",
      "Episode 260, Avg Reward: -40.02200000000043, PLoss: -16.192644119262695, VLoss: 1.0336147546768188\n",
      "Episode 270, Avg Reward: -38.740000000000435, PLoss: -13.623834609985352, VLoss: 3.1527493000030518\n",
      "Episode 280, Avg Reward: -39.77900000000043, PLoss: -10.167680740356445, VLoss: 2.1963083744049072\n",
      "Episode 290, Avg Reward: -39.77900000000043, PLoss: -10.131956100463867, VLoss: 8.233293533325195\n",
      "Episode 300, Avg Reward: -38.67800000000043, PLoss: 204.2556915283203, VLoss: 10507.916015625\n",
      "Episode 310, Avg Reward: -41.96300000000043, PLoss: -7.065249443054199, VLoss: 3.3716814517974854\n",
      "Episode 320, Avg Reward: -41.96300000000042, PLoss: -5.823724746704102, VLoss: 2.7320964336395264\n",
      "Episode 330, Avg Reward: -39.75100000000043, PLoss: -7.948293209075928, VLoss: 6.435652256011963\n",
      "Episode 340, Avg Reward: -40.96500000000044, PLoss: -4.503373146057129, VLoss: 3.2395143508911133\n",
      "Episode 350, Avg Reward: -40.96500000000044, PLoss: -5.586729049682617, VLoss: 4.177398681640625\n",
      "Episode 360, Avg Reward: -42.999000000000414, PLoss: -10.684809684753418, VLoss: 8.22668170928955\n",
      "Episode 370, Avg Reward: -44.28100000000041, PLoss: -1.193536400794983, VLoss: 6.952727317810059\n",
      "Episode 380, Avg Reward: -44.28100000000041, PLoss: -4.557665824890137, VLoss: 2.9134721755981445\n",
      "Episode 390, Avg Reward: -44.28100000000041, PLoss: -2.548088788986206, VLoss: 6.387602806091309\n",
      "Episode 400, Avg Reward: -46.52100000000039, PLoss: -4.716192245483398, VLoss: 4.793614387512207\n",
      "Episode 410, Avg Reward: -47.78800000000038, PLoss: -3.153209686279297, VLoss: 3.784989356994629\n",
      "Episode 420, Avg Reward: -47.78800000000038, PLoss: -0.8298148512840271, VLoss: 1.7083412408828735\n",
      "Episode 430, Avg Reward: -50.00000000000036, PLoss: -9.807418823242188, VLoss: 12.028887748718262\n",
      "Episode 440, Avg Reward: -50.00000000000036, PLoss: -0.8241182565689087, VLoss: 1.2439286708831787\n",
      "Episode 450, Avg Reward: -50.00000000000036, PLoss: -3.0910446643829346, VLoss: 6.297726631164551\n",
      "Episode 460, Avg Reward: -48.87800000000036, PLoss: -0.9442225694656372, VLoss: 2.733086347579956\n",
      "Episode 470, Avg Reward: -48.87800000000036, PLoss: -0.38498398661613464, VLoss: 4.672473907470703\n",
      "Episode 480, Avg Reward: -48.87800000000038, PLoss: -1.9977973699569702, VLoss: 2.9155216217041016\n",
      "Episode 490, Avg Reward: -48.87800000000038, PLoss: -0.565818727016449, VLoss: 4.115506649017334\n",
      "Episode 500, Avg Reward: -48.87800000000038, PLoss: -3.7720768451690674, VLoss: 4.659754276275635\n",
      "Episode 510, Avg Reward: -48.87800000000038, PLoss: -2.2232019901275635, VLoss: 9.798874855041504\n",
      "Episode 520, Avg Reward: -48.87800000000038, PLoss: -2.7951598167419434, VLoss: 3.5549709796905518\n",
      "Episode 530, Avg Reward: -48.87800000000038, PLoss: 1.8473693132400513, VLoss: 2.5249927043914795\n",
      "Episode 540, Avg Reward: -48.878000000000384, PLoss: -1.1159428358078003, VLoss: 1.6006267070770264\n",
      "Episode 550, Avg Reward: -48.878000000000384, PLoss: -0.8470249176025391, VLoss: 4.8411865234375\n",
      "Episode 560, Avg Reward: -50.00000000000036, PLoss: -0.053169894963502884, VLoss: 2.8753914833068848\n",
      "Episode 570, Avg Reward: -50.00000000000036, PLoss: -2.169713020324707, VLoss: 1.819049596786499\n",
      "Episode 580, Avg Reward: -50.00000000000036, PLoss: -1.8004423379898071, VLoss: 8.964079856872559\n",
      "Episode 590, Avg Reward: -50.00000000000036, PLoss: -4.47315788269043, VLoss: 9.213247299194336\n",
      "Episode 600, Avg Reward: -50.00000000000036, PLoss: -2.9631693363189697, VLoss: 3.911926746368408\n",
      "Episode 610, Avg Reward: -50.00000000000036, PLoss: -3.0460731983184814, VLoss: 7.937497615814209\n",
      "Episode 620, Avg Reward: -50.00000000000036, PLoss: -3.192972183227539, VLoss: 5.644010066986084\n",
      "Episode 630, Avg Reward: -50.00000000000036, PLoss: -0.1264376938343048, VLoss: 6.017439842224121\n",
      "Episode 640, Avg Reward: -50.00000000000036, PLoss: -1.334986686706543, VLoss: 1.5593024492263794\n",
      "Episode 650, Avg Reward: -50.00000000000036, PLoss: -2.044280767440796, VLoss: 3.8933932781219482\n",
      "Episode 660, Avg Reward: -50.00000000000036, PLoss: -0.7438497543334961, VLoss: 1.2588015794754028\n",
      "Episode 670, Avg Reward: -50.00000000000036, PLoss: -1.0736619234085083, VLoss: 1.6647491455078125\n",
      "Episode 680, Avg Reward: -50.00000000000036, PLoss: -3.135171413421631, VLoss: 6.732066631317139\n",
      "Episode 690, Avg Reward: -50.00000000000036, PLoss: -0.6379736065864563, VLoss: 4.529942512512207\n",
      "Episode 700, Avg Reward: -50.00000000000036, PLoss: -1.0849347114562988, VLoss: 3.3048131465911865\n",
      "Episode 710, Avg Reward: -50.00000000000036, PLoss: -1.6291179656982422, VLoss: 1.504677414894104\n",
      "Episode 720, Avg Reward: -50.00000000000036, PLoss: -1.076114296913147, VLoss: 1.4567641019821167\n",
      "Episode 730, Avg Reward: -50.00000000000036, PLoss: 0.3110364079475403, VLoss: 4.62082052230835\n",
      "Episode 740, Avg Reward: -50.00000000000036, PLoss: -0.9147080779075623, VLoss: 3.8779234886169434\n",
      "Episode 750, Avg Reward: -50.00000000000036, PLoss: 0.593645453453064, VLoss: 1.3335318565368652\n",
      "Episode 760, Avg Reward: -50.00000000000036, PLoss: 0.19787192344665527, VLoss: 3.3375418186187744\n",
      "Episode 770, Avg Reward: -50.00000000000036, PLoss: -0.49716633558273315, VLoss: 2.2384791374206543\n",
      "Episode 780, Avg Reward: -50.00000000000036, PLoss: -0.4177069365978241, VLoss: 1.1303507089614868\n",
      "Episode 790, Avg Reward: -50.00000000000036, PLoss: -0.2857136130332947, VLoss: 0.8728844523429871\n",
      "Episode 800, Avg Reward: -50.00000000000036, PLoss: -0.4523055851459503, VLoss: 2.6166133880615234\n",
      "Episode 810, Avg Reward: -50.00000000000036, PLoss: -6.587158679962158, VLoss: 6.151699066162109\n",
      "Episode 820, Avg Reward: -50.00000000000036, PLoss: -0.7471232414245605, VLoss: 1.9625258445739746\n",
      "Episode 830, Avg Reward: -50.00000000000036, PLoss: -0.6436485052108765, VLoss: 1.833327054977417\n",
      "Episode 840, Avg Reward: -50.00000000000036, PLoss: -1.0014113187789917, VLoss: 0.7671415209770203\n",
      "Episode 850, Avg Reward: -50.00000000000036, PLoss: -0.8886118531227112, VLoss: 0.768029510974884\n",
      "Episode 860, Avg Reward: -50.00000000000036, PLoss: -1.6003321409225464, VLoss: 2.876112699508667\n",
      "Episode 870, Avg Reward: -50.00000000000036, PLoss: -0.6063719987869263, VLoss: 1.4305593967437744\n",
      "Episode 880, Avg Reward: -50.00000000000036, PLoss: -0.8108444809913635, VLoss: 1.0289628505706787\n",
      "Episode 890, Avg Reward: -50.00000000000036, PLoss: 0.9427770376205444, VLoss: 4.292276382446289\n",
      "Episode 900, Avg Reward: -50.00000000000036, PLoss: 0.029176292940974236, VLoss: 1.936824917793274\n",
      "Episode 910, Avg Reward: -50.00000000000036, PLoss: -2.3332626819610596, VLoss: 2.1719322204589844\n",
      "Episode 920, Avg Reward: -50.00000000000036, PLoss: -1.1402958631515503, VLoss: 0.9101290702819824\n",
      "Episode 930, Avg Reward: -50.00000000000036, PLoss: -0.5965670347213745, VLoss: 1.5046049356460571\n",
      "Episode 940, Avg Reward: -50.00000000000036, PLoss: -0.46300122141838074, VLoss: 1.0591976642608643\n",
      "Episode 950, Avg Reward: -50.00000000000036, PLoss: -1.1960903406143188, VLoss: 1.7390716075897217\n",
      "Episode 960, Avg Reward: -50.00000000000036, PLoss: -0.990976870059967, VLoss: 0.7520273923873901\n",
      "Episode 970, Avg Reward: -50.00000000000036, PLoss: -1.4325200319290161, VLoss: 3.0900943279266357\n",
      "Episode 980, Avg Reward: -50.00000000000036, PLoss: -2.2186150550842285, VLoss: 2.0166404247283936\n",
      "Episode 990, Avg Reward: -50.00000000000036, PLoss: -1.0057897567749023, VLoss: 1.0588644742965698\n",
      "Episode 1000, Avg Reward: -50.00000000000036, PLoss: -0.3355245590209961, VLoss: 0.31403833627700806\n",
      "Episode 1010, Avg Reward: -50.00000000000036, PLoss: -1.0516639947891235, VLoss: 0.40845102071762085\n",
      "Episode 1020, Avg Reward: -50.00000000000036, PLoss: -0.26507043838500977, VLoss: 0.40432503819465637\n",
      "Episode 1030, Avg Reward: -50.00000000000036, PLoss: -2.5675759315490723, VLoss: 1.5905014276504517\n",
      "Episode 1040, Avg Reward: -50.00000000000036, PLoss: -1.2773369550704956, VLoss: 0.479928582906723\n",
      "Episode 1050, Avg Reward: -50.00000000000036, PLoss: -0.3305327594280243, VLoss: 0.6473408937454224\n",
      "Episode 1060, Avg Reward: -50.00000000000036, PLoss: -1.3142614364624023, VLoss: 0.8217454552650452\n",
      "Episode 1070, Avg Reward: -50.00000000000036, PLoss: -0.14755037426948547, VLoss: 1.8002972602844238\n",
      "Episode 1080, Avg Reward: -50.00000000000036, PLoss: -0.9790860414505005, VLoss: 0.6986735463142395\n",
      "Episode 1090, Avg Reward: -50.00000000000036, PLoss: -0.32275426387786865, VLoss: 0.4478242099285126\n",
      "Episode 1100, Avg Reward: -50.00000000000036, PLoss: -0.4828498065471649, VLoss: 0.3646736443042755\n",
      "Episode 1110, Avg Reward: -50.00000000000036, PLoss: -0.389946848154068, VLoss: 0.24862270057201385\n",
      "Episode 1120, Avg Reward: -50.00000000000036, PLoss: -0.12368449568748474, VLoss: 0.2312202900648117\n",
      "Episode 1130, Avg Reward: -50.00000000000036, PLoss: -0.18894390761852264, VLoss: 0.1250954270362854\n",
      "Episode 1140, Avg Reward: -50.00000000000036, PLoss: -0.33290427923202515, VLoss: 0.17181944847106934\n",
      "Episode 1150, Avg Reward: -50.00000000000036, PLoss: -0.7253592610359192, VLoss: 0.33666929602622986\n",
      "Episode 1160, Avg Reward: -50.00000000000036, PLoss: -2.4774749279022217, VLoss: 1.1703743934631348\n",
      "Episode 1170, Avg Reward: -50.00000000000036, PLoss: -1.0009095668792725, VLoss: 0.2791816294193268\n",
      "Episode 1180, Avg Reward: -50.00000000000036, PLoss: -0.6138532757759094, VLoss: 0.3454551696777344\n",
      "Episode 1190, Avg Reward: -50.00000000000036, PLoss: -0.2815653383731842, VLoss: 0.4335891902446747\n",
      "Episode 1200, Avg Reward: -50.00000000000036, PLoss: -0.3278825879096985, VLoss: 0.5948664546012878\n",
      "Episode 1210, Avg Reward: -50.00000000000036, PLoss: -0.2968229353427887, VLoss: 0.12654410302639008\n",
      "Episode 1220, Avg Reward: -50.00000000000036, PLoss: 0.064785897731781, VLoss: 0.05692978948354721\n",
      "Episode 1230, Avg Reward: -50.00000000000036, PLoss: -0.345845103263855, VLoss: 0.3076956570148468\n",
      "Episode 1240, Avg Reward: -50.00000000000036, PLoss: -0.4041059911251068, VLoss: 0.2700478136539459\n",
      "Episode 1250, Avg Reward: -50.00000000000036, PLoss: -0.40166226029396057, VLoss: 0.02918546088039875\n",
      "Episode 1260, Avg Reward: -50.00000000000036, PLoss: -1.1368207931518555, VLoss: 0.12031649053096771\n",
      "Episode 1270, Avg Reward: -50.00000000000036, PLoss: -0.5405910611152649, VLoss: 0.12072192132472992\n",
      "Episode 1280, Avg Reward: -50.00000000000036, PLoss: -0.5064067840576172, VLoss: 0.08565196394920349\n",
      "Episode 1290, Avg Reward: -50.00000000000036, PLoss: -0.29415827989578247, VLoss: 0.041684407740831375\n",
      "Episode 1300, Avg Reward: -50.00000000000036, PLoss: -0.3748020529747009, VLoss: 0.05557747557759285\n",
      "Episode 1310, Avg Reward: -50.00000000000036, PLoss: -0.2088472843170166, VLoss: 0.06953170895576477\n",
      "Episode 1320, Avg Reward: -50.00000000000036, PLoss: -0.7119231224060059, VLoss: 0.0331011638045311\n",
      "Episode 1330, Avg Reward: -50.00000000000036, PLoss: -0.281823068857193, VLoss: 0.12472907453775406\n",
      "Episode 1340, Avg Reward: -50.00000000000036, PLoss: -0.39606773853302, VLoss: 0.15503749251365662\n",
      "Episode 1350, Avg Reward: -50.00000000000036, PLoss: -0.37786033749580383, VLoss: 0.06530079990625381\n",
      "Episode 1360, Avg Reward: -50.00000000000036, PLoss: -0.5772496461868286, VLoss: 0.30207449197769165\n",
      "Episode 1370, Avg Reward: -50.00000000000036, PLoss: -0.28679442405700684, VLoss: 0.037804413586854935\n",
      "Episode 1380, Avg Reward: -50.00000000000036, PLoss: -0.1974550187587738, VLoss: 0.036112572997808456\n",
      "Episode 1390, Avg Reward: -50.00000000000036, PLoss: 0.11747321486473083, VLoss: 0.10668349266052246\n",
      "Episode 1400, Avg Reward: -50.00000000000036, PLoss: -0.3220880329608917, VLoss: 0.07769192010164261\n",
      "Episode 1410, Avg Reward: -50.00000000000036, PLoss: -0.061918582767248154, VLoss: 0.02989220805466175\n",
      "Episode 1420, Avg Reward: -50.00000000000036, PLoss: -0.1454588621854782, VLoss: 0.1540031135082245\n",
      "Episode 1430, Avg Reward: -50.00000000000036, PLoss: -0.2531392574310303, VLoss: 0.05939894914627075\n",
      "Episode 1440, Avg Reward: -50.00000000000036, PLoss: -0.11625588685274124, VLoss: 0.08851796388626099\n",
      "Episode 1450, Avg Reward: -50.00000000000036, PLoss: -0.6533469557762146, VLoss: 0.2591268718242645\n",
      "Episode 1460, Avg Reward: -50.00000000000036, PLoss: -0.7668008208274841, VLoss: 0.3445950150489807\n",
      "Episode 1470, Avg Reward: -50.00000000000036, PLoss: -0.6445668339729309, VLoss: 0.5736305117607117\n",
      "Episode 1480, Avg Reward: -50.00000000000036, PLoss: 0.1342899650335312, VLoss: 0.10035080462694168\n",
      "Episode 1490, Avg Reward: -50.00000000000036, PLoss: -1.776007890701294, VLoss: 0.622661828994751\n",
      "Episode 1500, Avg Reward: -50.00000000000036, PLoss: 0.1070738360285759, VLoss: 0.29205670952796936\n",
      "Episode 1510, Avg Reward: -50.00000000000036, PLoss: -0.2227001041173935, VLoss: 0.2846400737762451\n",
      "Episode 1520, Avg Reward: -50.00000000000036, PLoss: 0.13277168571949005, VLoss: 0.25099197030067444\n",
      "Episode 1530, Avg Reward: -50.00000000000036, PLoss: -0.5715810060501099, VLoss: 0.6641630530357361\n",
      "Episode 1540, Avg Reward: -50.00000000000036, PLoss: -0.4157771170139313, VLoss: 0.32131344079971313\n",
      "Episode 1550, Avg Reward: -50.00000000000036, PLoss: -0.19618579745292664, VLoss: 0.22229082882404327\n",
      "Episode 1560, Avg Reward: -50.00000000000036, PLoss: -0.059012044221162796, VLoss: 0.19737017154693604\n",
      "Episode 1570, Avg Reward: -50.00000000000036, PLoss: 1.0135916471481323, VLoss: 1.1355273723602295\n",
      "Episode 1580, Avg Reward: -50.00000000000036, PLoss: 0.5118364095687866, VLoss: 0.32077792286872864\n",
      "Episode 1590, Avg Reward: -50.00000000000036, PLoss: -0.30467087030410767, VLoss: 0.9284805655479431\n",
      "Episode 1600, Avg Reward: -50.00000000000036, PLoss: 0.13102546334266663, VLoss: 0.5089680552482605\n",
      "Episode 1610, Avg Reward: -50.00000000000036, PLoss: 0.19874036312103271, VLoss: 0.29429444670677185\n",
      "Episode 1620, Avg Reward: -50.00000000000036, PLoss: -0.4240977168083191, VLoss: 0.38315194845199585\n",
      "Episode 1630, Avg Reward: -50.00000000000036, PLoss: 0.571769118309021, VLoss: 1.2819218635559082\n",
      "Episode 1640, Avg Reward: -50.00000000000036, PLoss: -0.4522021412849426, VLoss: 0.3254542350769043\n",
      "Episode 1650, Avg Reward: -50.00000000000036, PLoss: 0.06399935483932495, VLoss: 0.17153674364089966\n",
      "Episode 1660, Avg Reward: -50.00000000000036, PLoss: -0.38257667422294617, VLoss: 0.34161442518234253\n",
      "Episode 1670, Avg Reward: -50.00000000000036, PLoss: 0.1694018840789795, VLoss: 0.2923678457736969\n",
      "Episode 1680, Avg Reward: -50.00000000000036, PLoss: -0.08746418356895447, VLoss: 0.23845987021923065\n",
      "Episode 1690, Avg Reward: -50.00000000000036, PLoss: 0.1998627483844757, VLoss: 1.5544756650924683\n",
      "Episode 1700, Avg Reward: -50.00000000000036, PLoss: 0.5229097604751587, VLoss: 1.383826494216919\n",
      "Episode 1710, Avg Reward: -50.00000000000036, PLoss: 0.009403666481375694, VLoss: 0.2661081850528717\n",
      "Episode 1720, Avg Reward: -50.00000000000036, PLoss: -0.328921914100647, VLoss: 0.8762243390083313\n",
      "Episode 1730, Avg Reward: -50.00000000000036, PLoss: 0.10184013843536377, VLoss: 0.34661293029785156\n",
      "Episode 1740, Avg Reward: -50.00000000000036, PLoss: 0.05816467106342316, VLoss: 0.5031614303588867\n",
      "Episode 1750, Avg Reward: -50.00000000000036, PLoss: -0.08240653574466705, VLoss: 1.1825838088989258\n",
      "Episode 1760, Avg Reward: -50.00000000000036, PLoss: -0.3900958299636841, VLoss: 1.2092829942703247\n",
      "Episode 1770, Avg Reward: -50.00000000000036, PLoss: 0.2653411626815796, VLoss: 1.6309680938720703\n",
      "Episode 1780, Avg Reward: -50.00000000000036, PLoss: 0.7088472247123718, VLoss: 0.7901633977890015\n",
      "Episode 1790, Avg Reward: -50.00000000000036, PLoss: -0.06491140276193619, VLoss: 0.41197434067726135\n",
      "Episode 1800, Avg Reward: -50.00000000000036, PLoss: -0.14962346851825714, VLoss: 0.46031704545021057\n",
      "Episode 1810, Avg Reward: -50.00000000000036, PLoss: -0.3336169123649597, VLoss: 1.5812615156173706\n",
      "Episode 1820, Avg Reward: -50.00000000000036, PLoss: 0.37516793608665466, VLoss: 0.8853676915168762\n",
      "Episode 1830, Avg Reward: -50.00000000000036, PLoss: 0.057342465966939926, VLoss: 0.3666611909866333\n",
      "Episode 1840, Avg Reward: -50.00000000000036, PLoss: -0.40755900740623474, VLoss: 1.5677841901779175\n",
      "Episode 1850, Avg Reward: -50.00000000000036, PLoss: -0.448032945394516, VLoss: 1.201841115951538\n",
      "Episode 1860, Avg Reward: -50.00000000000036, PLoss: -0.056693658232688904, VLoss: 0.9656088352203369\n",
      "Episode 1870, Avg Reward: -50.00000000000036, PLoss: -0.07304300367832184, VLoss: 0.5638238787651062\n",
      "Episode 1880, Avg Reward: -50.00000000000036, PLoss: 0.0007104912074282765, VLoss: 0.9118524789810181\n",
      "Episode 1890, Avg Reward: -50.00000000000036, PLoss: 0.6969689130783081, VLoss: 0.8425089716911316\n",
      "Episode 1900, Avg Reward: -50.00000000000036, PLoss: 0.3370686173439026, VLoss: 0.5645583868026733\n",
      "Episode 1910, Avg Reward: -50.00000000000036, PLoss: -0.07460580766201019, VLoss: 0.8793419003486633\n",
      "Episode 1920, Avg Reward: -50.00000000000036, PLoss: 0.5305933356285095, VLoss: 1.0527461767196655\n",
      "Episode 1930, Avg Reward: -50.00000000000036, PLoss: 0.08067718893289566, VLoss: 1.0626163482666016\n",
      "Episode 1940, Avg Reward: -50.00000000000036, PLoss: 0.06522221863269806, VLoss: 0.5375006198883057\n",
      "Episode 1950, Avg Reward: -50.00000000000036, PLoss: 0.07692959159612656, VLoss: 0.29163220524787903\n",
      "Episode 1960, Avg Reward: -50.00000000000036, PLoss: -0.3165837526321411, VLoss: 0.5397314429283142\n",
      "Episode 1970, Avg Reward: -50.00000000000036, PLoss: 0.02517905831336975, VLoss: 0.42524823546409607\n",
      "Episode 1980, Avg Reward: -50.00000000000036, PLoss: -0.028094837442040443, VLoss: 0.8438195586204529\n",
      "Episode 1990, Avg Reward: -50.00000000000036, PLoss: 0.2416672259569168, VLoss: 0.22406929731369019\n",
      "Episode 2000, Avg Reward: -50.00000000000036, PLoss: 0.104771189391613, VLoss: 0.2397851049900055\n",
      "Episode 2010, Avg Reward: -50.00000000000036, PLoss: -0.034109942615032196, VLoss: 0.1500682830810547\n",
      "Episode 2020, Avg Reward: -50.00000000000036, PLoss: 0.06581120938062668, VLoss: 1.0574702024459839\n",
      "Episode 2030, Avg Reward: -50.00000000000036, PLoss: -0.46805262565612793, VLoss: 0.6890638470649719\n",
      "Episode 2040, Avg Reward: -50.00000000000036, PLoss: -0.3852978050708771, VLoss: 0.665884792804718\n",
      "Episode 2050, Avg Reward: -50.00000000000036, PLoss: -0.29771703481674194, VLoss: 0.8709540963172913\n",
      "Episode 2060, Avg Reward: -50.00000000000036, PLoss: -0.3959817588329315, VLoss: 0.49178218841552734\n",
      "Episode 2070, Avg Reward: -50.00000000000036, PLoss: -0.15647169947624207, VLoss: 0.1299290806055069\n",
      "Episode 2080, Avg Reward: -50.00000000000036, PLoss: -0.20805980265140533, VLoss: 0.37025803327560425\n",
      "Episode 2090, Avg Reward: -50.00000000000036, PLoss: 0.09936857968568802, VLoss: 0.12690822780132294\n",
      "Episode 2100, Avg Reward: -50.00000000000036, PLoss: 0.023297328501939774, VLoss: 0.5874325037002563\n",
      "Episode 2110, Avg Reward: -50.00000000000036, PLoss: -0.031581811606884, VLoss: 0.11298014968633652\n",
      "Episode 2120, Avg Reward: -50.00000000000036, PLoss: -0.12324268370866776, VLoss: 0.6338837742805481\n",
      "Episode 2130, Avg Reward: -50.00000000000036, PLoss: -0.33607766032218933, VLoss: 0.7399672865867615\n",
      "Episode 2140, Avg Reward: -50.00000000000036, PLoss: 0.19362246990203857, VLoss: 0.4360361099243164\n",
      "Episode 2150, Avg Reward: -50.00000000000036, PLoss: -0.28175994753837585, VLoss: 0.125011146068573\n",
      "Episode 2160, Avg Reward: -50.00000000000036, PLoss: -0.03837132453918457, VLoss: 0.15155190229415894\n",
      "Episode 2170, Avg Reward: -50.00000000000036, PLoss: 0.32039010524749756, VLoss: 0.2698623538017273\n",
      "Episode 2180, Avg Reward: -50.00000000000036, PLoss: 0.08916588872671127, VLoss: 0.49371129274368286\n",
      "Episode 2190, Avg Reward: -50.00000000000036, PLoss: -0.04265640303492546, VLoss: 0.13330692052841187\n",
      "Episode 2200, Avg Reward: -50.00000000000036, PLoss: 0.15077818930149078, VLoss: 0.5346934199333191\n",
      "Episode 2210, Avg Reward: -50.00000000000036, PLoss: 0.2538225054740906, VLoss: 0.09738802164793015\n",
      "Episode 2220, Avg Reward: -50.00000000000036, PLoss: -0.3165678381919861, VLoss: 0.12505410611629486\n",
      "Episode 2230, Avg Reward: -50.00000000000036, PLoss: 0.21317999064922333, VLoss: 0.4049707353115082\n",
      "Episode 2240, Avg Reward: -50.00000000000036, PLoss: 0.006596741266548634, VLoss: 0.12948042154312134\n",
      "Episode 2250, Avg Reward: -50.00000000000036, PLoss: 0.20343784987926483, VLoss: 0.2824591398239136\n",
      "Episode 2260, Avg Reward: -50.00000000000036, PLoss: 0.27678412199020386, VLoss: 0.3752984404563904\n",
      "Episode 2270, Avg Reward: -50.00000000000036, PLoss: -0.17994889616966248, VLoss: 0.26810234785079956\n",
      "Episode 2280, Avg Reward: -50.00000000000036, PLoss: -0.036040641367435455, VLoss: 0.27922478318214417\n",
      "Episode 2290, Avg Reward: -50.00000000000036, PLoss: 0.2709437608718872, VLoss: 0.28358837962150574\n",
      "Episode 2300, Avg Reward: -50.00000000000036, PLoss: 0.007329212501645088, VLoss: 0.35613149404525757\n",
      "Episode 2310, Avg Reward: -50.00000000000036, PLoss: 0.37644824385643005, VLoss: 0.1677754521369934\n",
      "Episode 2320, Avg Reward: -50.00000000000036, PLoss: -0.02995116263628006, VLoss: 0.32485565543174744\n",
      "Episode 2330, Avg Reward: -50.00000000000036, PLoss: -0.1570449024438858, VLoss: 0.20999081432819366\n",
      "Episode 2340, Avg Reward: -50.00000000000036, PLoss: 0.42588314414024353, VLoss: 0.3030734658241272\n",
      "Episode 2350, Avg Reward: -50.00000000000036, PLoss: -0.042156100273132324, VLoss: 0.08791302144527435\n",
      "Episode 2360, Avg Reward: -50.00000000000036, PLoss: 0.021813375875353813, VLoss: 0.053215570747852325\n",
      "Episode 2370, Avg Reward: -50.00000000000036, PLoss: 0.09937969595193863, VLoss: 0.05244873836636543\n",
      "Episode 2380, Avg Reward: -50.00000000000036, PLoss: -0.2497342824935913, VLoss: 0.2799285054206848\n",
      "Episode 2390, Avg Reward: -50.00000000000036, PLoss: -0.2552468478679657, VLoss: 0.07077309489250183\n",
      "Episode 2400, Avg Reward: -50.00000000000036, PLoss: 0.010922214947640896, VLoss: 0.13002794981002808\n",
      "Episode 2410, Avg Reward: -50.00000000000036, PLoss: 0.15915431082248688, VLoss: 0.08309516310691833\n",
      "Episode 2420, Avg Reward: -50.00000000000036, PLoss: 0.09411095082759857, VLoss: 0.14641088247299194\n",
      "Episode 2430, Avg Reward: -50.00000000000036, PLoss: 0.20375612378120422, VLoss: 0.043809857219457626\n",
      "Episode 2440, Avg Reward: -50.00000000000036, PLoss: 0.07588977366685867, VLoss: 0.16041721403598785\n",
      "Episode 2450, Avg Reward: -50.00000000000036, PLoss: -0.11494556814432144, VLoss: 0.0627530887722969\n",
      "Episode 2460, Avg Reward: -50.00000000000036, PLoss: -0.0606163889169693, VLoss: 0.12049367278814316\n",
      "Episode 2470, Avg Reward: -50.00000000000036, PLoss: -0.1326705366373062, VLoss: 0.15686291456222534\n",
      "Episode 2480, Avg Reward: -50.00000000000036, PLoss: -0.27216824889183044, VLoss: 0.18854792416095734\n",
      "Episode 2490, Avg Reward: -50.00000000000036, PLoss: 0.004627545364201069, VLoss: 0.041560593992471695\n",
      "Episode 2500, Avg Reward: -50.00000000000036, PLoss: 0.19154296815395355, VLoss: 0.10070668905973434\n",
      "Episode 2510, Avg Reward: -50.00000000000036, PLoss: 0.17734146118164062, VLoss: 0.07490557432174683\n",
      "Episode 2520, Avg Reward: -50.00000000000036, PLoss: -0.08595581352710724, VLoss: 0.10522342473268509\n",
      "Episode 2530, Avg Reward: -50.00000000000036, PLoss: 0.0070130834355950356, VLoss: 0.05955493077635765\n",
      "Episode 2540, Avg Reward: -50.00000000000036, PLoss: -0.1445017009973526, VLoss: 0.07620029896497726\n",
      "Episode 2550, Avg Reward: -50.00000000000036, PLoss: 0.24496367573738098, VLoss: 0.07738646864891052\n",
      "Episode 2560, Avg Reward: -50.00000000000036, PLoss: -0.01665308140218258, VLoss: 0.02968805655837059\n",
      "Episode 2570, Avg Reward: -50.00000000000036, PLoss: 0.0723937600851059, VLoss: 0.025573918595910072\n",
      "Episode 2580, Avg Reward: -50.00000000000036, PLoss: -0.2135850042104721, VLoss: 0.07802923023700714\n",
      "Episode 2590, Avg Reward: -50.00000000000036, PLoss: 0.050342585891485214, VLoss: 0.06143401190638542\n",
      "Episode 2600, Avg Reward: -50.00000000000036, PLoss: 0.001193217234686017, VLoss: 0.03455884009599686\n",
      "Episode 2610, Avg Reward: -50.00000000000036, PLoss: -0.06742553412914276, VLoss: 0.04770471900701523\n",
      "Episode 2620, Avg Reward: -50.00000000000036, PLoss: 0.09830602258443832, VLoss: 0.09205648303031921\n",
      "Episode 2630, Avg Reward: -50.00000000000036, PLoss: -0.3726804256439209, VLoss: 0.10434877872467041\n",
      "Episode 2640, Avg Reward: -50.00000000000036, PLoss: 0.16857433319091797, VLoss: 0.01451815664768219\n",
      "Episode 2650, Avg Reward: -50.00000000000036, PLoss: 0.021049467846751213, VLoss: 0.03105597011744976\n",
      "Episode 2660, Avg Reward: -50.00000000000036, PLoss: -0.18085601925849915, VLoss: 0.029697639867663383\n",
      "Episode 2670, Avg Reward: -50.00000000000036, PLoss: -0.15203115344047546, VLoss: 0.0842861756682396\n",
      "Episode 2680, Avg Reward: -50.00000000000036, PLoss: -0.09257058799266815, VLoss: 0.026896553114056587\n",
      "Episode 2690, Avg Reward: -50.00000000000036, PLoss: 0.051846109330654144, VLoss: 0.08027595281600952\n",
      "Episode 2700, Avg Reward: -50.00000000000036, PLoss: 0.028704429045319557, VLoss: 0.04509282857179642\n",
      "Episode 2710, Avg Reward: -50.00000000000036, PLoss: 0.10292072594165802, VLoss: 0.034881990402936935\n",
      "Episode 2720, Avg Reward: -50.00000000000036, PLoss: 0.1800822615623474, VLoss: 0.08801741898059845\n",
      "Episode 2730, Avg Reward: -50.00000000000036, PLoss: -0.04899432510137558, VLoss: 0.04375148192048073\n",
      "Episode 2740, Avg Reward: -50.00000000000036, PLoss: -0.07978305965662003, VLoss: 0.09460838884115219\n",
      "Episode 2750, Avg Reward: -50.00000000000036, PLoss: 0.159540057182312, VLoss: 0.06846436113119125\n",
      "Episode 2760, Avg Reward: -50.00000000000036, PLoss: -0.15104590356349945, VLoss: 0.028075238689780235\n",
      "Episode 2770, Avg Reward: -50.00000000000036, PLoss: 0.047580786049366, VLoss: 0.041711799800395966\n",
      "Episode 2780, Avg Reward: -50.00000000000036, PLoss: -0.19810619950294495, VLoss: 0.07297854125499725\n",
      "Episode 2790, Avg Reward: -50.00000000000036, PLoss: 0.030504349619150162, VLoss: 0.01863439381122589\n",
      "Episode 2800, Avg Reward: -50.00000000000036, PLoss: -0.06136531010270119, VLoss: 0.019667331129312515\n",
      "Episode 2810, Avg Reward: -50.00000000000036, PLoss: 0.006433662958443165, VLoss: 0.012028603814542294\n",
      "Episode 2820, Avg Reward: -50.00000000000036, PLoss: 0.036818359047174454, VLoss: 0.007250399328768253\n",
      "Episode 2830, Avg Reward: -50.00000000000036, PLoss: -0.05156813561916351, VLoss: 0.007250386290252209\n",
      "Episode 2840, Avg Reward: -50.00000000000036, PLoss: 0.09072308242321014, VLoss: 0.05784771218895912\n",
      "Episode 2850, Avg Reward: -50.00000000000036, PLoss: 0.0440535731613636, VLoss: 0.016899367794394493\n",
      "Episode 2860, Avg Reward: -50.00000000000036, PLoss: -0.02290281467139721, VLoss: 0.02146247774362564\n",
      "Episode 2870, Avg Reward: -50.00000000000036, PLoss: -0.003925783094018698, VLoss: 0.0203902255743742\n",
      "Episode 2880, Avg Reward: -50.00000000000036, PLoss: -0.005126240197569132, VLoss: 0.01586179994046688\n",
      "Episode 2890, Avg Reward: -50.00000000000036, PLoss: -0.12664420902729034, VLoss: 0.06504097580909729\n",
      "Episode 2900, Avg Reward: -50.00000000000036, PLoss: -0.12546642124652863, VLoss: 0.04248071834445\n",
      "Episode 2910, Avg Reward: -50.00000000000036, PLoss: -0.04215998202562332, VLoss: 0.017035581171512604\n",
      "Episode 2920, Avg Reward: -50.00000000000036, PLoss: 0.13299264013767242, VLoss: 0.06996709108352661\n",
      "Episode 2930, Avg Reward: -50.00000000000036, PLoss: -0.22122281789779663, VLoss: 0.06321340799331665\n",
      "Episode 2940, Avg Reward: -50.00000000000036, PLoss: 0.01281062327325344, VLoss: 0.005849969573318958\n",
      "Episode 2950, Avg Reward: -50.00000000000036, PLoss: 0.11415818333625793, VLoss: 0.010106843896210194\n",
      "Episode 2960, Avg Reward: -50.00000000000036, PLoss: 0.0012630022829398513, VLoss: 0.008915122598409653\n",
      "Episode 2970, Avg Reward: -50.00000000000036, PLoss: -0.032476745545864105, VLoss: 0.05471909046173096\n",
      "Episode 2980, Avg Reward: -50.00000000000036, PLoss: 0.01410334650427103, VLoss: 0.006342638283967972\n",
      "Episode 2990, Avg Reward: -50.00000000000036, PLoss: 0.09657030552625656, VLoss: 0.030279170721769333\n",
      "Episode 3000, Avg Reward: -50.00000000000036, PLoss: 0.05806310847401619, VLoss: 0.011001458391547203\n",
      "Episode 3010, Avg Reward: -50.00000000000036, PLoss: 0.0563393160700798, VLoss: 0.012979601509869099\n",
      "Episode 3020, Avg Reward: -50.00000000000036, PLoss: 0.11391106992959976, VLoss: 0.02327091060578823\n",
      "Episode 3030, Avg Reward: -50.00000000000036, PLoss: -0.03796716779470444, VLoss: 0.011365310288965702\n",
      "Episode 3040, Avg Reward: -50.00000000000036, PLoss: -0.5311414003372192, VLoss: 0.0792800635099411\n",
      "Episode 3050, Avg Reward: -50.00000000000036, PLoss: -0.051098305732011795, VLoss: 0.013509842567145824\n",
      "Episode 3060, Avg Reward: -50.00000000000036, PLoss: 0.008123967796564102, VLoss: 0.005210694391280413\n",
      "Episode 3070, Avg Reward: -50.00000000000036, PLoss: 0.012437444180250168, VLoss: 0.00698334164917469\n",
      "Episode 3080, Avg Reward: -50.00000000000036, PLoss: -0.0015762237599119544, VLoss: 0.009824325330555439\n",
      "Episode 3090, Avg Reward: -50.00000000000036, PLoss: -0.05796205252408981, VLoss: 0.009292101487517357\n",
      "Episode 3100, Avg Reward: -50.00000000000036, PLoss: -0.14289253950119019, VLoss: 0.06577599048614502\n",
      "Episode 3110, Avg Reward: -50.00000000000036, PLoss: -0.0003397218824829906, VLoss: 0.0126161128282547\n",
      "Episode 3120, Avg Reward: -50.00000000000036, PLoss: -0.057571493089199066, VLoss: 0.020958803594112396\n",
      "Episode 3130, Avg Reward: -50.00000000000036, PLoss: -0.0022702638525515795, VLoss: 0.03806513175368309\n",
      "Episode 3140, Avg Reward: -50.00000000000036, PLoss: 0.009171401150524616, VLoss: 0.0043390472419559956\n",
      "Episode 3150, Avg Reward: -50.00000000000036, PLoss: 0.0045875986106693745, VLoss: 0.028854483738541603\n",
      "Episode 3160, Avg Reward: -50.00000000000036, PLoss: 0.08833791315555573, VLoss: 0.06044745445251465\n",
      "Episode 3170, Avg Reward: -50.00000000000036, PLoss: 0.000335303891915828, VLoss: 0.02134578675031662\n",
      "Episode 3180, Avg Reward: -50.00000000000036, PLoss: -0.07923062890768051, VLoss: 0.013388707302510738\n",
      "Episode 3190, Avg Reward: -50.00000000000036, PLoss: 0.03871676325798035, VLoss: 0.006838476285338402\n",
      "Episode 3200, Avg Reward: -50.00000000000036, PLoss: -0.11187435686588287, VLoss: 0.04150315001606941\n",
      "Episode 3210, Avg Reward: -50.00000000000036, PLoss: -0.013471047393977642, VLoss: 0.004822061862796545\n",
      "Episode 3220, Avg Reward: -50.00000000000036, PLoss: -0.27911245822906494, VLoss: 0.03920184075832367\n",
      "Episode 3230, Avg Reward: -50.00000000000036, PLoss: 0.03963995724916458, VLoss: 0.013628732413053513\n",
      "Episode 3240, Avg Reward: -50.00000000000036, PLoss: 0.024021098390221596, VLoss: 0.008838432841002941\n",
      "Episode 3250, Avg Reward: -50.00000000000036, PLoss: -0.0016231888439506292, VLoss: 0.015271701849997044\n",
      "Episode 3260, Avg Reward: -50.00000000000036, PLoss: -0.018499432131648064, VLoss: 0.019846390932798386\n",
      "Episode 3270, Avg Reward: -50.00000000000036, PLoss: -0.13503383100032806, VLoss: 0.058063581585884094\n",
      "Episode 3280, Avg Reward: -50.00000000000036, PLoss: 0.08662570267915726, VLoss: 0.008947116322815418\n",
      "Episode 3290, Avg Reward: -50.00000000000036, PLoss: -0.015946242958307266, VLoss: 0.017189238220453262\n",
      "Episode 3300, Avg Reward: -50.00000000000036, PLoss: -0.006011913996189833, VLoss: 0.007578057236969471\n",
      "Episode 3310, Avg Reward: -50.00000000000036, PLoss: -0.1029239073395729, VLoss: 0.028615562245249748\n",
      "Episode 3320, Avg Reward: -50.00000000000036, PLoss: 0.000698623713105917, VLoss: 0.02659079246222973\n",
      "Episode 3330, Avg Reward: -50.00000000000036, PLoss: -0.07015779614448547, VLoss: 0.02338835410773754\n",
      "Episode 3340, Avg Reward: -50.00000000000036, PLoss: 0.1798260658979416, VLoss: 0.03428065404295921\n",
      "Episode 3350, Avg Reward: -50.00000000000036, PLoss: 0.006024808622896671, VLoss: 0.014109107665717602\n",
      "Episode 3360, Avg Reward: -50.00000000000036, PLoss: 0.0032718197908252478, VLoss: 0.006551193539053202\n",
      "Episode 3370, Avg Reward: -50.00000000000036, PLoss: -0.09844712167978287, VLoss: 0.010030156001448631\n",
      "Episode 3380, Avg Reward: -50.00000000000036, PLoss: -0.026667049154639244, VLoss: 0.021368635818362236\n",
      "Episode 3390, Avg Reward: -50.00000000000036, PLoss: 0.09212104976177216, VLoss: 0.032780446112155914\n",
      "Episode 3400, Avg Reward: -50.00000000000036, PLoss: 0.05907105281949043, VLoss: 0.01609143428504467\n",
      "Episode 3410, Avg Reward: -50.00000000000036, PLoss: 0.09449645131826401, VLoss: 0.026839571073651314\n",
      "Episode 3420, Avg Reward: -50.00000000000036, PLoss: -0.21391518414020538, VLoss: 0.04290961101651192\n",
      "Episode 3430, Avg Reward: -50.00000000000036, PLoss: 0.09642186760902405, VLoss: 0.017015142366290092\n",
      "Episode 3440, Avg Reward: -50.00000000000036, PLoss: -0.1809813231229782, VLoss: 0.0403599850833416\n",
      "Episode 3450, Avg Reward: -50.00000000000036, PLoss: -0.12794256210327148, VLoss: 0.05083930492401123\n",
      "Episode 3460, Avg Reward: -50.00000000000036, PLoss: 0.024818990379571915, VLoss: 0.030084777623414993\n",
      "Episode 3470, Avg Reward: -50.00000000000036, PLoss: 0.0596182756125927, VLoss: 0.013179381377995014\n",
      "Episode 3480, Avg Reward: -50.00000000000036, PLoss: -0.15764929354190826, VLoss: 0.04311148822307587\n",
      "Episode 3490, Avg Reward: -50.00000000000036, PLoss: 0.07834665477275848, VLoss: 0.01712176203727722\n",
      "Episode 3500, Avg Reward: -50.00000000000036, PLoss: 0.025647088885307312, VLoss: 0.015873417258262634\n",
      "Episode 3510, Avg Reward: -50.00000000000036, PLoss: -0.017594769597053528, VLoss: 0.01965978741645813\n",
      "Episode 3520, Avg Reward: -50.00000000000036, PLoss: -0.012809338048100471, VLoss: 0.0075018201023340225\n",
      "Episode 3530, Avg Reward: -50.00000000000036, PLoss: -0.1646403968334198, VLoss: 0.041469212621450424\n",
      "Episode 3540, Avg Reward: -50.00000000000036, PLoss: -0.02109118178486824, VLoss: 0.0056570712476968765\n",
      "Episode 3550, Avg Reward: -50.00000000000036, PLoss: 0.03286891430616379, VLoss: 0.018448857590556145\n",
      "Episode 3560, Avg Reward: -50.00000000000036, PLoss: -0.1615714579820633, VLoss: 0.01623070240020752\n",
      "Episode 3570, Avg Reward: -50.00000000000036, PLoss: -0.19781865179538727, VLoss: 0.025269318372011185\n",
      "Episode 3580, Avg Reward: -50.00000000000036, PLoss: -0.10456755757331848, VLoss: 0.010740053839981556\n",
      "Episode 3590, Avg Reward: -50.00000000000036, PLoss: 0.016619069501757622, VLoss: 0.003907088190317154\n",
      "Episode 3600, Avg Reward: -50.00000000000036, PLoss: 0.025764770805835724, VLoss: 0.005355231463909149\n",
      "Episode 3610, Avg Reward: -50.00000000000036, PLoss: -0.3661818206310272, VLoss: 0.034415025264024734\n",
      "Episode 3620, Avg Reward: -50.00000000000036, PLoss: 0.026831122115254402, VLoss: 0.00472221290692687\n",
      "Episode 3630, Avg Reward: -50.00000000000036, PLoss: -0.09296420961618423, VLoss: 0.016174061223864555\n",
      "Episode 3640, Avg Reward: -50.00000000000036, PLoss: 0.0850578024983406, VLoss: 0.019789613783359528\n",
      "Episode 3650, Avg Reward: -50.00000000000036, PLoss: -0.12859709560871124, VLoss: 0.03511562943458557\n",
      "Episode 3660, Avg Reward: -50.00000000000036, PLoss: -0.2888372838497162, VLoss: 0.03573322296142578\n",
      "Episode 3670, Avg Reward: -50.00000000000036, PLoss: 0.012966277077794075, VLoss: 0.011075722053647041\n",
      "Episode 3680, Avg Reward: -50.00000000000036, PLoss: 0.03615857660770416, VLoss: 0.019538018852472305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward_threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:170\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, env_wrapper, max_episodes, max_steps, reward_threshold, update_frequency)\u001b[0m\n\u001b[1;32m    167\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    168\u001b[0m transitions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m    171\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m    172\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env_wrapper\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/sise/home/amaruy/Deep Learning/DeepRL/actorcritic.py:93\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     92\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 93\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network(state)\n\u001b[1;32m     94\u001b[0m     m \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[1;32m     95\u001b[0m     action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/distributions/categorical.py:64\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     63\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/logdeep/lib/python3.9/site-packages/torch/distributions/distribution.py:52\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), param), lazy_property):\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constraint\u001b[38;5;241m.\u001b[39mcheck(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param))\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has invalid values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(param))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_100\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage 100\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisodes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(results['Episode'], results['Reward'], label='Reward')\n",
    "plt.plot(results['Episode'], results['Average_100'], label='Average 100')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'results/{config[\"experiment\"]}', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
