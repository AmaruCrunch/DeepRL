{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actorcritic import ActorCriticAgent, ContinuousActorCriticAgent, EnvironmentWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'experiment': 'MountainCarContinuous',\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 64], \n",
    "    'lr_actor': 0.001,\n",
    "    'lr_critic': 0.0005,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'MountainCarContinuous-v0',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 80.0,\n",
    "    'max_episodes': 10000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 500,\n",
    "    'discrete': True\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def __init__(self, env, num_actions=5):\n",
    "        super().__init__(env)\n",
    "        self.action_space = num_actions\n",
    "        # Define the discrete action boundaries\n",
    "        self.action_boundaries = np.linspace(-1, 1, num_actions)\n",
    "        self.ticker = 100000\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert the discrete action into a continuous action\n",
    "        continuous_action = [self.discretize_action(action)]\n",
    "\n",
    "\n",
    "        # Step using the continuous action\n",
    "        state, reward, done, _, info = self.env.step(continuous_action)\n",
    "\n",
    "        reward = 0\n",
    "        if state[0] >= 0.45:\n",
    "            print('success')\n",
    "            reward += 1\n",
    "\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "        return padded_state, reward, done, info\n",
    "\n",
    "    def discretize_action(self, action):\n",
    "        # Ensure the action is within the valid range\n",
    "        action = max(0, min(action, self.action_space - 1))\n",
    "        return self.action_boundaries[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env, num_actions=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ActorCriticAgent\n",
    "agent = ActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 0.0, PLoss: -0.02809300273656845, VLoss: 2.346990368096158e-05\n",
      "success\n",
      "success\n",
      "Episode 10, Avg Reward: 0.18181818181818182, PLoss: -0.13501030206680298, VLoss: 4.6662262320751324e-05\n",
      "success\n",
      "Episode 20, Avg Reward: 0.14285714285714285, PLoss: 0.006202287040650845, VLoss: 0.0008403310785070062\n",
      "success\n",
      "success\n",
      "Episode 30, Avg Reward: 0.16129032258064516, PLoss: 0.8085190653800964, VLoss: 0.7923123240470886\n",
      "Episode 40, Avg Reward: 0.12195121951219512, PLoss: -0.0646931454539299, VLoss: 0.0007449614349752665\n",
      "success\n",
      "success\n",
      "Episode 50, Avg Reward: 0.13725490196078433, PLoss: -0.03795450180768967, VLoss: 0.001049244194291532\n",
      "success\n",
      "success\n",
      "success\n",
      "Episode 60, Avg Reward: 0.16393442622950818, PLoss: 0.8319472670555115, VLoss: 0.6701138019561768\n",
      "Episode 70, Avg Reward: 0.14084507042253522, PLoss: -0.2892712652683258, VLoss: 0.002572599332779646\n",
      "success\n",
      "Episode 80, Avg Reward: 0.13580246913580246, PLoss: -0.010653910227119923, VLoss: 0.0015055539552122355\n",
      "Episode 90, Avg Reward: 0.12087912087912088, PLoss: -0.061698637902736664, VLoss: 0.0007909191772341728\n",
      "Episode 100, Avg Reward: 0.11, PLoss: -0.11322181671857834, VLoss: 0.0016552004963159561\n",
      "Episode 110, Avg Reward: 0.09, PLoss: -0.062152132391929626, VLoss: 0.0006663805106654763\n",
      "success\n",
      "Episode 120, Avg Reward: 0.09, PLoss: -0.08453884720802307, VLoss: 0.0004364817577879876\n",
      "Episode 130, Avg Reward: 0.07, PLoss: -0.01880279742181301, VLoss: 0.0002612102252896875\n",
      "success\n",
      "Episode 140, Avg Reward: 0.08, PLoss: -0.042147569358348846, VLoss: 4.1035385947907344e-05\n",
      "Episode 150, Avg Reward: 0.06, PLoss: -0.04221644997596741, VLoss: 0.002101869322359562\n",
      "Episode 160, Avg Reward: 0.03, PLoss: -0.039064038544893265, VLoss: 0.0003325092257000506\n",
      "success\n",
      "success\n",
      "Episode 170, Avg Reward: 0.05, PLoss: 0.4256153106689453, VLoss: 0.6312196254730225\n",
      "Episode 180, Avg Reward: 0.04, PLoss: -0.06526391953229904, VLoss: 0.0009199364576488733\n",
      "Episode 190, Avg Reward: 0.04, PLoss: -0.10578246414661407, VLoss: 0.002887273207306862\n",
      "Episode 200, Avg Reward: 0.04, PLoss: -0.1291387975215912, VLoss: 0.0014456115895882249\n",
      "Episode 210, Avg Reward: 0.04, PLoss: 0.0037423798348754644, VLoss: 0.0010015420848503709\n",
      "Episode 220, Avg Reward: 0.03, PLoss: 0.0028655920177698135, VLoss: 0.000383965641958639\n",
      "Episode 230, Avg Reward: 0.03, PLoss: -0.003969838842749596, VLoss: 8.289600373245776e-05\n",
      "Episode 240, Avg Reward: 0.02, PLoss: 0.02768610790371895, VLoss: 0.0002278180472785607\n",
      "success\n",
      "Episode 250, Avg Reward: 0.03, PLoss: 0.9879967570304871, VLoss: 0.6288619637489319\n",
      "Episode 260, Avg Reward: 0.03, PLoss: -0.025261178612709045, VLoss: 0.0007233047508634627\n",
      "Episode 270, Avg Reward: 0.01, PLoss: -0.02908257208764553, VLoss: 0.0011668303050100803\n",
      "Episode 280, Avg Reward: 0.01, PLoss: 0.011102463118731976, VLoss: 0.0006768308812752366\n",
      "Episode 290, Avg Reward: 0.01, PLoss: -0.04281602054834366, VLoss: 0.0005764159723185003\n",
      "Episode 300, Avg Reward: 0.01, PLoss: -0.06472339481115341, VLoss: 0.0036050262860953808\n",
      "success\n",
      "Episode 310, Avg Reward: 0.02, PLoss: -0.0029691378585994244, VLoss: 0.0008319079643115401\n",
      "Episode 320, Avg Reward: 0.02, PLoss: -0.019157161936163902, VLoss: 0.0003956587752327323\n",
      "Episode 330, Avg Reward: 0.02, PLoss: -0.03643973916769028, VLoss: 0.0010974901961162686\n",
      "Episode 340, Avg Reward: 0.02, PLoss: 0.009048177860677242, VLoss: 2.8105429009883665e-05\n",
      "Episode 350, Avg Reward: 0.01, PLoss: -0.0021971219684928656, VLoss: 0.00010574308544164523\n",
      "Episode 360, Avg Reward: 0.01, PLoss: -0.0026589767076075077, VLoss: 3.905131234205328e-05\n",
      "Episode 370, Avg Reward: 0.01, PLoss: -0.03564993664622307, VLoss: 0.0006121547776274383\n",
      "Episode 380, Avg Reward: 0.01, PLoss: 0.006753693800419569, VLoss: 1.8776114302454516e-05\n",
      "Episode 390, Avg Reward: 0.01, PLoss: -0.01661861129105091, VLoss: 0.0009792926721274853\n",
      "Episode 400, Avg Reward: 0.01, PLoss: 0.00437969109043479, VLoss: 1.1090208317909855e-05\n",
      "Episode 410, Avg Reward: 0.0, PLoss: 0.004464259836822748, VLoss: 2.7899402994080447e-05\n",
      "Episode 420, Avg Reward: 0.0, PLoss: -0.01170158851891756, VLoss: 0.00023657936253584921\n",
      "Episode 430, Avg Reward: 0.0, PLoss: 0.013653463684022427, VLoss: 9.728277291287668e-06\n",
      "success\n",
      "Episode 440, Avg Reward: 0.01, PLoss: -0.05510818958282471, VLoss: 0.00040360246202908456\n",
      "Episode 450, Avg Reward: 0.01, PLoss: 0.01722452975809574, VLoss: 2.5778983399504796e-05\n",
      "Episode 460, Avg Reward: 0.01, PLoss: -0.01615225523710251, VLoss: 5.510576556844171e-06\n",
      "Episode 470, Avg Reward: 0.01, PLoss: -0.021655209362506866, VLoss: 0.0003952340339310467\n",
      "Episode 480, Avg Reward: 0.01, PLoss: -0.034815713763237, VLoss: 0.00016827965737320483\n",
      "Episode 490, Avg Reward: 0.01, PLoss: -0.036925092339515686, VLoss: 0.0011337392497807741\n",
      "Episode 500, Avg Reward: 0.01, PLoss: -0.001995844766497612, VLoss: 3.77823889721185e-05\n",
      "Episode 510, Avg Reward: 0.01, PLoss: 0.00709129823371768, VLoss: 1.8163851564168e-05\n",
      "Episode 520, Avg Reward: 0.01, PLoss: 0.0012407583417370915, VLoss: 7.265851309057325e-05\n",
      "Episode 530, Avg Reward: 0.01, PLoss: -0.000571582408156246, VLoss: 1.0484824997547548e-05\n",
      "Episode 540, Avg Reward: 0.0, PLoss: 0.004674775991588831, VLoss: 3.4000881896645296e-06\n",
      "Episode 550, Avg Reward: 0.0, PLoss: 0.00013905482774134725, VLoss: 9.230556088368758e-07\n",
      "Episode 560, Avg Reward: 0.0, PLoss: 0.0008577940170653164, VLoss: 3.7056906876387075e-06\n",
      "Episode 570, Avg Reward: 0.0, PLoss: -0.0019843813497573137, VLoss: 1.0003720490203705e-05\n",
      "Episode 580, Avg Reward: 0.0, PLoss: -0.0018804756691679358, VLoss: 3.005093049068819e-06\n",
      "Episode 590, Avg Reward: 0.0, PLoss: 0.0006157690077088773, VLoss: 5.684729785571108e-06\n",
      "Episode 600, Avg Reward: 0.0, PLoss: 0.000981357879936695, VLoss: 5.523319032363361e-06\n",
      "Episode 610, Avg Reward: 0.0, PLoss: -0.0003196783654857427, VLoss: 8.370689465664327e-05\n",
      "Episode 620, Avg Reward: 0.0, PLoss: -0.011878346092998981, VLoss: 0.00014763914805371314\n",
      "Episode 630, Avg Reward: 0.0, PLoss: -0.0028924057260155678, VLoss: 1.0243324140901677e-05\n",
      "Episode 640, Avg Reward: 0.0, PLoss: 0.003755053272470832, VLoss: 1.5870313291088678e-05\n",
      "Episode 650, Avg Reward: 0.0, PLoss: -0.0015147558879107237, VLoss: 3.0414832963288063e-06\n",
      "Episode 660, Avg Reward: 0.0, PLoss: -0.007139273919165134, VLoss: 0.00013848314119968563\n",
      "Episode 670, Avg Reward: 0.0, PLoss: -0.007062002085149288, VLoss: 0.0001700629509286955\n",
      "Episode 680, Avg Reward: 0.0, PLoss: -0.0018097020220011473, VLoss: 2.364713282076991e-06\n",
      "Episode 690, Avg Reward: 0.0, PLoss: 0.002465564990416169, VLoss: 1.0825309800566174e-05\n",
      "Episode 700, Avg Reward: 0.0, PLoss: -0.0009476147824898362, VLoss: 1.634161890251562e-06\n",
      "Episode 710, Avg Reward: 0.0, PLoss: -0.0012559234164655209, VLoss: 2.3435310140484944e-05\n",
      "Episode 720, Avg Reward: 0.0, PLoss: -0.0005586674669757485, VLoss: 1.8592908190839808e-06\n",
      "Episode 730, Avg Reward: 0.0, PLoss: 0.002415647730231285, VLoss: 5.3469179874809925e-06\n",
      "Episode 740, Avg Reward: 0.0, PLoss: 0.0017338264733552933, VLoss: 2.2550561880052555e-06\n",
      "Episode 750, Avg Reward: 0.0, PLoss: 0.000782536924816668, VLoss: 2.1265418581606355e-06\n",
      "Episode 760, Avg Reward: 0.0, PLoss: -0.00995592214167118, VLoss: 1.5579020328004844e-05\n",
      "Episode 770, Avg Reward: 0.0, PLoss: -0.0010658702813088894, VLoss: 6.665043656539638e-06\n",
      "Episode 780, Avg Reward: 0.0, PLoss: -0.00014130212366580963, VLoss: 2.038880893451278e-06\n",
      "Episode 790, Avg Reward: 0.0, PLoss: 0.002795726992189884, VLoss: 1.1530358278832864e-05\n",
      "Episode 800, Avg Reward: 0.0, PLoss: 0.00243582995608449, VLoss: 3.3541125503688818e-06\n",
      "Episode 810, Avg Reward: 0.0, PLoss: 0.0011844116961583495, VLoss: 3.947992581743165e-07\n",
      "Episode 820, Avg Reward: 0.0, PLoss: 0.0024298650678247213, VLoss: 2.7964922537648818e-06\n",
      "Episode 830, Avg Reward: 0.0, PLoss: -0.00027803319972008467, VLoss: 5.173658337298548e-06\n",
      "Episode 840, Avg Reward: 0.0, PLoss: -0.0013497091131284833, VLoss: 2.591939619378536e-06\n",
      "Episode 850, Avg Reward: 0.0, PLoss: 0.001026432728394866, VLoss: 6.195906280481722e-06\n",
      "Episode 860, Avg Reward: 0.0, PLoss: 0.0032036597840487957, VLoss: 3.874855849517189e-07\n",
      "Episode 870, Avg Reward: 0.0, PLoss: -0.003051102627068758, VLoss: 4.0541341149946675e-05\n",
      "Episode 880, Avg Reward: 0.0, PLoss: -0.0003666721750050783, VLoss: 8.362567882613803e-07\n",
      "Episode 890, Avg Reward: 0.0, PLoss: -0.0018860211130231619, VLoss: 4.7642479330534115e-06\n",
      "Episode 900, Avg Reward: 0.0, PLoss: 0.0010403088526800275, VLoss: 8.671632713230792e-06\n",
      "Episode 910, Avg Reward: 0.0, PLoss: 0.001835462637245655, VLoss: 2.511363163648639e-06\n",
      "Episode 920, Avg Reward: 0.0, PLoss: -0.00042944264714606106, VLoss: 2.066047727566911e-06\n",
      "Episode 930, Avg Reward: 0.0, PLoss: 0.0008231193060055375, VLoss: 4.434398590547062e-07\n",
      "Episode 940, Avg Reward: 0.0, PLoss: -0.0016264935256913304, VLoss: 1.0925396054517478e-05\n",
      "Episode 950, Avg Reward: 0.0, PLoss: -0.00028017142903991044, VLoss: 2.0412555841176072e-06\n",
      "Episode 960, Avg Reward: 0.0, PLoss: 0.000903246458619833, VLoss: 9.71694134932477e-07\n",
      "Episode 970, Avg Reward: 0.0, PLoss: 0.0014787587570026517, VLoss: 4.832489253203676e-07\n",
      "Episode 980, Avg Reward: 0.0, PLoss: -0.0012536145513877273, VLoss: 8.601386980444659e-06\n",
      "Episode 990, Avg Reward: 0.0, PLoss: 0.001301307464018464, VLoss: 7.2643078965484165e-06\n",
      "Episode 1000, Avg Reward: 0.0, PLoss: 0.0023898128420114517, VLoss: 5.099053623780492e-07\n",
      "Episode 1010, Avg Reward: 0.0, PLoss: -0.0020738388411700726, VLoss: 1.0202236808254384e-05\n",
      "Episode 1020, Avg Reward: 0.0, PLoss: -7.383510092040524e-05, VLoss: 8.978549885796383e-07\n",
      "Episode 1030, Avg Reward: 0.0, PLoss: 0.001642857096157968, VLoss: 7.128694051061757e-06\n",
      "Episode 1040, Avg Reward: 0.0, PLoss: -0.000768061785493046, VLoss: 8.72489999892423e-06\n",
      "Episode 1050, Avg Reward: 0.0, PLoss: -0.00026156066451221704, VLoss: 1.0801890084621846e-06\n",
      "Episode 1060, Avg Reward: 0.0, PLoss: -0.00032067965366877615, VLoss: 7.499140224354051e-07\n",
      "Episode 1070, Avg Reward: 0.0, PLoss: -0.0007236163364723325, VLoss: 7.88277066021692e-06\n",
      "Episode 1080, Avg Reward: 0.0, PLoss: 0.00039756158366799355, VLoss: 4.073924174008425e-06\n",
      "Episode 1090, Avg Reward: 0.0, PLoss: 0.00019614322809502482, VLoss: 3.492028554319404e-05\n",
      "Episode 1100, Avg Reward: 0.0, PLoss: -0.0007840744801796973, VLoss: 9.615242788640899e-07\n",
      "Episode 1110, Avg Reward: 0.0, PLoss: -4.0363276639254764e-05, VLoss: 3.13185091727064e-06\n",
      "Episode 1120, Avg Reward: 0.0, PLoss: -0.0009688948630355299, VLoss: 1.3489559023582842e-05\n",
      "Episode 1130, Avg Reward: 0.0, PLoss: -0.00017433019820600748, VLoss: 9.703398973215371e-06\n"
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env, num_actions=config['action_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(results['Episode'], results['Reward'], label='Reward')\n",
    "plt.plot(results['Episode'], results['Average_100'], label='Average 100')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'results/{config[\"experiment\"]}', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
