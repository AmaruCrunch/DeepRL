{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "- Test update policy every 1 step\n",
    "- try a network with smaller value network and bigger policy\n",
    "- try a network with bigger value network and smaller policy\n",
    "- learning rates\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actorcritic import ActorCriticAgent, EnvironmentWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'experiment': 'MountainCarContinuous',\n",
    "    'device': 'cuda',\n",
    "    'state_size': 6, \n",
    "    'action_size': 3,\n",
    "    'hidden_sizes': [64, 64], \n",
    "    'lr_actor': 0.001,\n",
    "    'lr_critic': 0.0005,\n",
    "    'verbosity': 10,\n",
    "    'env_name': 'MountainCarContinuous-v0',\n",
    "    'gamma': 0.99, \n",
    "    'reward_threshold': 80.0,\n",
    "    'max_episodes': 5000,\n",
    "    'max_steps': 500,\n",
    "    'update_frequency': 500,\n",
    "    'discrete': True\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCWrapper(EnvironmentWrapper):\n",
    "    def __init__(self, env, num_actions=3):\n",
    "        super().__init__(env)\n",
    "        self.action_space = num_actions\n",
    "        # Define the discrete action boundaries\n",
    "        self.action_boundaries = np.linspace(-1, 1, num_actions)\n",
    "        self.ticker = 0\n",
    "        self.succeeded = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert the discrete action into a continuous action\n",
    "        continuous_action = [self.discretize_action(action)]\n",
    "\n",
    "\n",
    "        # Step using the continuous action\n",
    "        state, reward, done, _, info = self.env.step(continuous_action)\n",
    "\n",
    "        if self.succeeded < 10: # handicap\n",
    "            reward = abs(state[1]) * 0.1 # moving is good\n",
    "            \n",
    "            if state[0] >= 0.45:\n",
    "                reward += 100\n",
    "                self.succeeded += 1\n",
    "        \n",
    "\n",
    "        # Pad state to match target_state_size\n",
    "        padded_state = np.append(state, np.zeros(self.target_state_size - len(state)))\n",
    "        \n",
    "        self.ticker += 1\n",
    "\n",
    "        return padded_state, reward, done, info\n",
    "    \n",
    "\n",
    "\n",
    "    def discretize_action(self, action):\n",
    "        # Ensure the action is within the valid range\n",
    "        action = max(0, min(action, self.action_space - 1))\n",
    "        return self.action_boundaries[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(config['env_name'])\n",
    "env_wrapper = MCCWrapper(env, num_actions=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ActorCriticAgent\n",
    "agent = ActorCriticAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: 0.4180766788389978, PLoss: 0.24419857561588287, VLoss: 0.0004944583051837981\n",
      "Episode 10, Avg Reward: 0.6881008790885941, PLoss: 0.0584699921309948, VLoss: 0.0004620781692210585\n",
      "Episode 20, Avg Reward: 0.636839961839267, PLoss: 0.037031374871730804, VLoss: 0.000879179744515568\n",
      "Episode 30, Avg Reward: 0.6109605053597765, PLoss: -0.25794145464897156, VLoss: 0.000250643992330879\n",
      "Episode 40, Avg Reward: 0.6175547899389272, PLoss: -0.1533486396074295, VLoss: 0.00020580833370331675\n",
      "Episode 50, Avg Reward: 0.5975033236424929, PLoss: -0.25046008825302124, VLoss: 0.0002084196894429624\n",
      "Episode 60, Avg Reward: 0.5747839960813022, PLoss: -0.2340220957994461, VLoss: 0.00021947978530079126\n",
      "Episode 70, Avg Reward: 0.5789288467937755, PLoss: 0.17237350344657898, VLoss: 0.0007483710069209337\n",
      "Episode 80, Avg Reward: 0.584081420450645, PLoss: -0.10213816165924072, VLoss: 0.0001845803635660559\n",
      "Episode 90, Avg Reward: 0.5740652143469852, PLoss: -0.13329079747200012, VLoss: 0.000254740851232782\n",
      "Episode 100, Avg Reward: 0.5723837793860462, PLoss: 0.05846516042947769, VLoss: 0.0007560885278508067\n",
      "Episode 110, Avg Reward: 0.5639680520388959, PLoss: 0.0036978283897042274, VLoss: 0.00033086244366131723\n",
      "Episode 120, Avg Reward: 0.5647637186454662, PLoss: -0.016721300780773163, VLoss: 0.0002946718013845384\n",
      "Episode 130, Avg Reward: 0.556801165012413, PLoss: -0.0869133397936821, VLoss: 0.0002571993973106146\n",
      "Episode 140, Avg Reward: 0.5585517306528324, PLoss: 0.0675605908036232, VLoss: 0.0009451686637476087\n",
      "Episode 150, Avg Reward: 0.5685158081709824, PLoss: 0.02604413591325283, VLoss: 0.0005502680432982743\n",
      "Episode 160, Avg Reward: 0.5743511065574666, PLoss: -0.10226065665483475, VLoss: 0.00024851912166923285\n",
      "Episode 170, Avg Reward: 0.5796994012213609, PLoss: -0.12107579410076141, VLoss: 0.0002332978619961068\n",
      "Episode 180, Avg Reward: 0.5704606856969177, PLoss: 0.12649039924144745, VLoss: 0.0004755924455821514\n",
      "Episode 190, Avg Reward: 0.5690107613083214, PLoss: 0.25229182839393616, VLoss: 0.0007933393935672939\n",
      "Episode 200, Avg Reward: 0.5690250367880672, PLoss: -0.10640030354261398, VLoss: 0.0002484536380507052\n",
      "Episode 210, Avg Reward: 0.5583083783932401, PLoss: -0.349508136510849, VLoss: 0.00027375834179110825\n",
      "Episode 220, Avg Reward: 0.5702389321092027, PLoss: 0.08893020451068878, VLoss: 0.000359292229404673\n",
      "Episode 230, Avg Reward: 0.568392222572781, PLoss: -0.2942908704280853, VLoss: 0.00025463380734436214\n",
      "Episode 240, Avg Reward: 0.5570956776077771, PLoss: -0.2844177186489105, VLoss: 0.00022131926380097866\n",
      "Episode 250, Avg Reward: 0.5482266313914798, PLoss: 0.14847928285598755, VLoss: 0.00037164700916036963\n",
      "Episode 260, Avg Reward: 0.553980271294942, PLoss: 0.2943985164165497, VLoss: 0.0007937511545605958\n",
      "Episode 270, Avg Reward: 0.5500346469643185, PLoss: 0.4025997519493103, VLoss: 0.0013956265756860375\n",
      "Episode 280, Avg Reward: 0.5589870363886678, PLoss: 0.23992115259170532, VLoss: 0.0010409994283691049\n",
      "Episode 290, Avg Reward: 0.5684400017600079, PLoss: 0.08101031184196472, VLoss: 0.0005550978821702302\n",
      "Episode 300, Avg Reward: 0.566041162102547, PLoss: -0.1954103410243988, VLoss: 0.0002406615240033716\n",
      "Episode 310, Avg Reward: 0.5627144739609246, PLoss: -0.18825362622737885, VLoss: 0.0001467714610043913\n",
      "Episode 320, Avg Reward: 0.544402486083907, PLoss: 0.052725598216056824, VLoss: 0.0004062479711137712\n",
      "Episode 330, Avg Reward: 0.5573798274845863, PLoss: 0.07323136180639267, VLoss: 0.0007617367664352059\n",
      "Episode 340, Avg Reward: 0.5572529586961589, PLoss: -0.23296937346458435, VLoss: 0.0002300899795955047\n",
      "Episode 350, Avg Reward: 0.5457201495100498, PLoss: -0.05486224964261055, VLoss: 0.00021088235371280462\n"
     ]
    }
   ],
   "source": [
    "results = agent.train(env_wrapper, max_episodes=config['max_episodes'], max_steps=config['max_steps'], reward_threshold=config['reward_threshold'], update_frequency=config['update_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_100\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage 100\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisodes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(results['Episode'], results['Reward'], label='Reward')\n",
    "plt.plot(results['Episode'], results['Average_100'], label='Average 100')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'results/{config[\"experiment\"]}', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
